# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Train a YOLOv5 model on a custom dataset.
Models and datasets download automatically from the latest YOLOv5 release.

Usage - Single-GPU training:
    $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (recommended)#ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè®­ç»ƒ
    $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch #ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼‰

Usage - Multi-GPU DDP training:
    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights yolov5s.pt --img 640 --device 0,1,2,3 #ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒ

Models:     https://github.com/ultralytics/yolov5/tree/master/models
Datasets:   https://github.com/ultralytics/yolov5/tree/master/data
Tutorial:   https://docs.ultralytics.com/yolov5/tutorials/train_custom_data
"""

#å¯¼å…¥çš„æ˜¯ç”¨æˆ·å®‰è£…çš„ä¾èµ–åŒ…
import argparse
import math
import os
import random
import sys
import time
from copy import deepcopy
from datetime import datetime
from pathlib import Path
 
import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import yaml
from torch.optim import lr_scheduler
from tqdm import tqdm
 
# 2.è·¯å¾„è§£æ
FILE = Path(__file__).resolve()
# è·å–å½“å‰æ–‡ä»¶train.pyçš„ç»å¯¹è·¯å¾„ï¼Œå¹¶èµ‹å€¼ç»™FILEå˜é‡ï¼Œ__file__æŒ‡train.py
ROOT = FILE.parents[0]  # YOLOv5 root directory
# è·å–å½“å‰æ–‡ä»¶çš„çˆ¶çº§ç›®å½•å¹¶èµ‹å€¼ç»™ROOTå˜é‡ï¼Œä¹Ÿå°±æ˜¯yolov5çš„ç»å¯¹è·¯å¾„
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
# å°†å½“å‰æ–‡ä»¶çš„çˆ¶çº§ç›®å½•æ·»åŠ åˆ°Pythonè§£é‡Šå™¨æœç´¢æ¨¡å—çš„è·¯å¾„åˆ—è¡¨ä¸­ï¼Œ
# è¿™æ ·å¯ä»¥é€šè¿‡importå¯¼å…¥çˆ¶çº§ç›®å½•ä¸‹çš„æ¨¡å—ï¼Œ
# sys.pathå­˜å‚¨äº†pythonæ¨¡å—çš„è·¯å¾„ï¼Œæ˜¯ä¸€ä¸ªåˆ—è¡¨
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative
# å°†å½“å‰æ–‡ä»¶çš„çˆ¶ç›®å½•è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
 
# 3.å¯¼å…¥æ¨¡å—
import val  # for end-of-epoch mAP
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors
from utils.autobatch import check_train_batch_size
from utils.callbacks import Callbacks
from utils.dataloaders import create_dataloader
from utils.downloads import attempt_download, is_url
from utils.general import (LOGGER, check_amp, check_dataset, check_file, check_git_status, check_img_size,
                           check_requirements, check_suffix, check_yaml, colorstr, get_latest_run, increment_path,
                           init_seeds, intersect_dicts, labels_to_class_weights, labels_to_image_weights, methods,
                           one_cycle, print_args, print_mutation, strip_optimizer, yaml_save)
from utils.loggers import Loggers
from utils.loggers.wandb.wandb_utils import check_wandb_resume
from utils.loss import ComputeLoss
from utils.metrics import fitness
from utils.plots import plot_evolve, plot_labels
from utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, select_device, smart_DDP, smart_optimizer,
                               smart_resume, torch_distributed_zero_first)


def train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary
    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \
        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \
        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze
    # åˆ›å»ºä¸€äº›å˜é‡å­˜å‚¨è§£æåˆ°çš„å‚æ•°
    callbacks.run('on_pretrain_routine_start') # è®­ç»ƒè¿‡ç¨‹å³å°†å¼€å§‹ï¼Œç”¨äºæ‰§è¡Œä¸€äº›è®­ç»ƒé¢„å¤„ç†æˆ–åˆå§‹åŒ–çš„æ“ä½œ
    # å›è°ƒå‡½æ•°ï¼šå¯¹è±¡åˆ—è¡¨æˆ–å¯¹è±¡ï¼Œç”¨äºåœ¨ä¸åŒçš„è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¸€äº›æ•°æ®çš„å¤„ç†æˆ–å±•ç¤º
 
    # Directories ä¿å­˜ç›®å½•
    w = save_dir / 'weights'  # weights dir æƒé‡ä¿å­˜çš„è·¯å¾„
    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir
    last, best = w / 'last.pt', w / 'best.pt' # ä¿å­˜æœ€åä¸€æ¬¡å’Œæœ€å¥½çš„ä¸€æ¬¡æƒé‡
 
    # Hyperparameters è¶…å‚æ•°
    # hypæ˜¯å­—å…¸å½¢å¼æˆ–å­—ç¬¦ä¸²å½¢å¼
    # è‹¥æ˜¯å­—å…¸å½¢å¼ï¼Œåˆ™ä¿å­˜äº†è¶…å‚æ•°çš„é”®å€¼å¯¹ï¼Œæ— éœ€è§£æ
    # è‹¥æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ä¿å­˜äº†ä»¥yamlæ ¼å¼ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    if isinstance(hyp, str):  # å¦‚æœhypæ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯è·¯å¾„æ ¼å¼ï¼Œéœ€è¦è§£æ
        with open(hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # load hyps dict å°†yamlæ–‡ä»¶è§£ææˆå­—å…¸å½¢å¼ï¼Œå¹¶åŠ è½½åˆ°hpyå˜é‡ä¸­
    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))  # æ‰“å°æ—¥å¿—ä¿¡æ¯ï¼šè¶…å‚æ•°ä¿¡æ¯
    opt.hyp = hyp.copy()  # for saving hyps to checkpoints
 
    # Save run settings
    if not evolve: # å¦‚æœä¸ä½¿ç”¨è¿›åŒ–è¶…å‚æ•°
        yaml_save(save_dir / 'hyp.yaml', hyp) # å°†è¶…å‚æ•°ä¿¡æ¯ä»¥yamlå½¢å¼ä¿å­˜
        yaml_save(save_dir / 'opt.yaml', vars(opt)) # å°†å‚æ•°ä¿¡æ¯è½¬æ¢ä¸ºå­—å…¸å½¢å¼å¹¶ä»¥yamlå½¢å¼ä¿å­˜
 
    # Loggers æ—¥å¿—è®°å½• å…·ä½“å¯ä»¥çœ‹ä¸€ä¸‹Loggersè¿™ä¸ªç±»
    # ä½¿ç”¨å“ªç§å½¢å¼è¿›è¡Œè®°å½• clearmlå½¢å¼æˆ–wandbå½¢å¼
    # è®°å½•ç»“æœå°±æ˜¯result.pngä¸­çš„ç»“æœ
    data_dict = None
    if RANK in {-1, 0}:
        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance
        if loggers.clearml:
            data_dict = loggers.clearml.data_dict  # None if no ClearML dataset or filled in by ClearML
        if loggers.wandb:
            data_dict = loggers.wandb.data_dict
            if resume:
                weights, epochs, hyp, batch_size = opt.weights, opt.epochs, opt.hyp, opt.batch_size
 
        # Register actions
        # å°†Loggersç±»ä¸­å®šä¹‰çš„éç§æœ‰æ–¹æ³•æ³¨å†Œåˆ°å›è°ƒå‡½æ•°ä¸­ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå›è°ƒå‡½æ•°è¿›è¡Œè°ƒç”¨ã€‚
        for k in methods(loggers):
            callbacks.register_action(k, callback=getattr(loggers, k))
 
    # Config  é…ç½®
    plots = not evolve and not opt.noplots  # create plots è®­ç»ƒè¿‡ç¨‹ä¸­ç”»çº¿
    cuda = device.type != 'cpu'
    # åˆå§‹åŒ–éšæœºç§å­ï¼Œè®­ç»ƒè¿‡ç¨‹å¯å¤ç°
    init_seeds(opt.seed + 1 + RANK, deterministic=True)
    # ä»¥ä¸‹ä¸¤è¡Œæ˜¯è¿›è¡Œåˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œæ£€æŸ¥æ•°æ®é›†çš„æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œå°†æ£€æŸ¥ç»“æœä¿å­˜åœ¨data_dictå˜é‡ä¸­ï¼Œdata_dictæ˜¯å­—å…¸å½¢å¼ï¼Œå†…å®¹æ˜¯æ ¹æ®dataæ–‡ä»¶å¤¹ä¸‹æŒ‡å®šçš„yamlæ–‡ä»¶è§£æå¾—åˆ°çš„
    with torch_distributed_zero_first(LOCAL_RANK):
        data_dict = data_dict or check_dataset(data)  # check if None
    # è·å¾—è®­ç»ƒæ•°æ®é›†è·¯å¾„å’ŒéªŒè¯æ•°æ®é›†è·¯å¾„
    train_path, val_path = data_dict['train'], data_dict['val']
    nc = 1 if single_cls else int(data_dict['nc'])  # number of classes è·å–ç±»åˆ«æ•°
    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names è·å–ç±»åˆ«å
    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check æ£€æŸ¥ç±»åˆ«æ•°æ˜¯å¦ç›¸ç­‰
    is_coco = isinstance(val_path, str) and val_path.endswith('coco/val2017.txt')  # COCO dataset åˆ¤æ–­æ˜¯å¦æ˜¯cocoæ•°æ®é›†
 
    # Model
    check_suffix(weights, '.pt')  # check weights æ£€æŸ¥æƒé‡æ–‡ä»¶æ˜¯å¦ä»¥.ptç»“å°¾
    pretrained = weights.endswith('.pt') # å­˜å‚¨é¢„è®­ç»ƒæƒé‡
    if pretrained:
        with torch_distributed_zero_first(LOCAL_RANK):
            weights = attempt_download(weights)  # download if not found locally æ£€æŸ¥æœ‰æ²¡æœ‰æƒé‡ï¼Œæ²¡æœ‰å°±ä¸‹è½½
        ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak
        # åŠ è½½æƒé‡ï¼Œå¹¶å°†æƒé‡å‚æ•°ä»¥å­—å…¸å½¢å¼å­˜å‚¨åˆ°ckptå˜é‡ä¸­
        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create åˆ›å»ºæ–°æ¨¡å‹
        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys
        # excludeåˆ—è¡¨åŒ…å«äº†éœ€è¦æ’é™¤çš„å‚æ•°åç§°ï¼Œè¿™äº›å‚æ•°éœ€è¦é‡æ–°è®­ç»ƒï¼Œä¿è¯å®ƒä»¬ä¸å—é¢„è®­ç»ƒæ¨¡å‹çš„å½±å“
        # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ‰äº›å‚æ•°æ˜¯å¯ä»¥ä»é¢„è®­ç»ƒæ¨¡å‹ç›´æ¥åŠ è½½ï¼Œè€Œæœ‰äº›å‚æ•°åˆ™éœ€è¦é‡æ–°è®­ç»ƒ
        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œå¹¶è½¬æ¢ä¸ºfloatç±»å‹ï¼Œä»¥å­—å…¸å½¢å¼å­˜å‚¨åœ¨csdå˜é‡ä¸­
        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect
        # å¯¹é¢„è®­ç»ƒæ¨¡å‹å’Œæ–°æ¨¡å‹ä¸­çš„æƒé‡å‚æ•°å–äº¤é›†ï¼Œå¹¶æ’é™¤excludeä¸­çš„å†…å®¹
        # äº¤é›†ä¸­çš„å‚æ•°åœ¨åç»­æƒé‡æ›´æ–°æ—¶ä¼šç”¨åˆ°ï¼Œexcludeä¸­çš„å†…å®¹éœ€è¦è®­ç»ƒ
        model.load_state_dict(csd, strict=False)  # load
        # æŠŠæ‰€æœ‰ç›¸åŒçš„å‚æ•°åŠ è½½åˆ°æ–°æ¨¡å‹ é¢„è®­ç»ƒæ¨¡å‹ä¸­æŸäº›å‚æ•°å¯¹è‡ªå·±çš„æ¨¡å‹æ˜¯æœ‰å¸®åŠ©çš„
        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report è®°å½•ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­è½¬ç§»äº†å¤šå°‘å‚æ•°ï¼Œä»¥åŠæ–°æ¨¡å‹ä¸­ä¸€å…±æœ‰å¤šå°‘å‚æ•°
    else: # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæƒé‡ï¼Œåˆ™ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹
        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create
    amp = check_amp(model)  # check AMP æ£€æŸ¥æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯ç”¨åˆ™ampè¿”å›Ture
 
    # Freeze æ§åˆ¶å†»ç»“å“ªäº›å±‚ åˆ›å»ºäº†ä¸€ä¸ªå†»ç»“å‚æ•°åˆ—è¡¨freezeï¼Œç”¨äºå†³å®šéœ€è¦å†»ç»“çš„å‚æ•°å±‚æ•°
    freeze = [f'model.{x}.' for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # layers to freeze
    # å‚æ•°freezeæ˜¯ä¸€ä¸ªæ•´æ•°åˆ—è¡¨æˆ–å•ä¸ªæ•´æ•°ï¼Œç”¨äºè¡¨ç¤ºéœ€è¦å†»ç»“çš„å‚æ•°å±‚æ•°
    # å¦‚æœfreezeæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œåˆ™è¡¨ç¤ºè¦å†»ç»“æ¨¡å‹ä¸­çš„å‰freezeå±‚å‚æ•°
    # å¦‚æœfreezeæ˜¯ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œåˆ™è¡¨ç¤ºè¦å†»ç»“æ¨¡å‹ä¸­åŒ…å«åœ¨è¿™äº›å±‚æ•°ä¸­çš„æ‰€æœ‰å‚æ•°
    for k, v in model.named_parameters(): # éå†æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°ï¼Œè¿”å›é”®å’Œå€¼
        v.requires_grad = True  # train all layers æ›´æ–°æ¢¯åº¦è®¾ä¸ºTrueï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°å‚æ•°
        # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)
        if any(x in k for x in freeze): # å¦‚æœxè¡¨ç¤ºçš„å‚æ•°æ‰€åœ¨çš„å±‚åœ¨éœ€è¦å†»ç»“çš„å±‚ä¸­
            LOGGER.info(f'freezing {k}')
            v.requires_grad = False # åˆ™å°†å¯¹åº”çš„æ›´æ–°æ¢¯åº¦è®¾ä¸ºFalseï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ›´æ–°å‚æ•°
 
    # Image size è°ƒæ•´è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸
    gs = max(int(model.stride.max()), 32)  # grid size (max stride)
    # è®¡ç®—å½“å‰æ¨¡å‹çš„æœ€å¤§strideï¼Œå¹¶å–strideå’Œ32çš„æœ€å¤§å€¼
    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple
    # æ£€æŸ¥ç”¨æˆ·æŒ‡å®šçš„è¾“å…¥å›¾åƒçš„å¤§å°æ˜¯å¦æ˜¯æ­¥é•¿gsçš„æ•´æ•°å€ï¼Œè°ƒæ•´è¾“å…¥æ¨¡å‹çš„å›¾ç‰‡å¤§å°
 
    # Batch size bs=-1è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—batch_sizeå¤§å°
    if RANK == -1 and batch_size == -1:  # single-GPU only, estimate best batch size
        batch_size = check_train_batch_size(model, imgsz, amp)
        loggers.on_params_update({"batch_size": batch_size})
 
    # Optimizer ä¼˜åŒ–å™¨
    nbs = 64  # nominal batch size åä¹‰ä¸Šbs ä¸€æ‰¹æ•°æ®è¶Šå¤šï¼Œæ¢¯åº¦æ›´æ–°æ–¹å‘è¶Šå‡†ç¡®
    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing
    # è®¡ç®—æ¢¯åº¦ç´¯è®¡çš„æ­¥æ•°accumulate
    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay
    # æ ¹æ®nbsã€bsã€accumulateè°ƒæ•´æƒé‡è¡°å‡ç³»æ•°
    # ç”¨äºæ§åˆ¶è¿‡æ‹Ÿåˆçš„æƒé‡è¡°å‡å‚æ•°ï¼Œä½¿å…¶åœ¨ä¸åŒçš„æ‰¹æ¬¡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¸‹ï¼Œå…·æœ‰ç›¸ä¼¼çš„ä½œç”¨ï¼Œé¿å…å½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆæœ
    optimizer = smart_optimizer(model, opt.optimizer, hyp['lr0'], hyp['momentum'], hyp['weight_decay'])
    # åˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨å¯¹è±¡ï¼Œä»¥ä¾¿åœ¨æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µä¸­é€šè¿‡æ¢¯åº¦åå‘ä¼ æ’­ç®—æ³•
    # å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œè¾¾åˆ°è®­ç»ƒé›†ä¼˜åŒ–çš„ç›®çš„
    # smart_optimizerä¸­å®šä¹‰äº†ä¼˜åŒ–å™¨ç±»å‹ä»¥åŠè¦ä¼˜åŒ–çš„å‚æ•°å˜é‡
 
    # Scheduler å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥
    if opt.cos_lr: # ä½™å¼¦å‡½æ•°å­¦ä¹ ç‡è°ƒæ•´
        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']
    else:  # çº¿æ€§å‡½æ•°å­¦ä¹ ç‡è°ƒæ•´
        lf = lambda x: (1 - x / epochs) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)
 
    # EMA
    # åˆ›å»ºä¸€ä¸ªæŒ‡æ•°ç§»åŠ¨å¹³å‡æ¨¡å‹å¯¹è±¡ï¼ˆemaï¼‰ï¼Œä»¥ç”¨äºåœ¨è®­ç»ƒæœŸé—´è®¡ç®—æ¨¡å‹æƒé‡çš„æŒ‡æ•°æ»‘åŠ¨å¹³å‡å€¼ï¼Œå¹¶åœ¨éªŒè¯æœŸé—´ä½¿ç”¨è¿™ä¸ªå¹³å‡å€¼å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°
    ema = ModelEMA(model) if RANK in {-1, 0} else None
 
    # Resume æ¢å¤è®­ç»ƒ
    best_fitness, start_epoch = 0.0, 0
 # è¿™ä¸¤ä¸ªå˜é‡åˆ†åˆ«ç”¨æ¥å­˜å‚¨æ¨¡å‹è®­ç»ƒåˆ°ç›®å‰ä¸ºæ­¢åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°çš„æœ€ä½³æ•ˆæœï¼ˆbest_fitnessï¼‰ä»¥åŠæ¨¡å‹å¼€å§‹è®­ç»ƒçš„è½®æ•°ï¼ˆstart_epochï¼‰
    if pretrained: # å¦‚æœéœ€è¦é¢„è®­ç»ƒ
        if resume: # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ–­ç‚¹å¤„æ¢å¤è®­ç»ƒ
            best_fitness, start_epoch, epochs = smart_resume(ckpt, optimizer, ema, weights, epochs, resume)
    # è°ƒç”¨ smart_resume() å‡½æ•°åŠ è½½è®­ç»ƒè½®æ¬¡ä¿¡æ¯ï¼Œä»¥åŠä¹‹å‰è®­ç»ƒè¿‡ç¨‹ä¸­å­˜å‚¨çš„æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€æŒ‡æ•°æ»‘åŠ¨å¹³å‡æ¨¡å‹ç­‰çŠ¶æ€ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ¢å¤åˆ°å½“å‰çŠ¶æ€ä¸­
        del ckpt, csd # åˆ é™¤ä¸éœ€è¦çš„å˜é‡é‡Šæ”¾å†…å­˜ç©ºé—´
 
    # DP mode  å¤šgpuè®­ç»ƒ
    if cuda and RANK == -1 and torch.cuda.device_count() > 1:
        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\n'
                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')
        model = torch.nn.DataParallel(model)
 
    # SyncBatchNorm ä¸å¤šåˆ†å¸ƒå¼è®­ç»ƒç›¸å…³
    # è‹¥é‡‡ç”¨å¤šåˆ†å¸ƒå¼è®­ç»ƒåˆ™å°†banchnormå±‚æ›¿æ¢ä¸ºSyncBatchNorm
    if opt.sync_bn and cuda and RANK != -1:
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
        LOGGER.info('Using SyncBatchNorm()')
 
    # Trainloader è®­ç»ƒæ•°æ®åŠ è½½
    # æ ¹æ®è¾“å…¥å‚æ•°ä»æŒ‡å®šè·¯å¾„åŠ è½½è®­ç»ƒæ•°æ®ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ•°æ®åŠ è½½å™¨å’Œä¸€ä¸ªæ•°æ®é›†å¯¹è±¡ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ
    train_loader, dataset = create_dataloader(train_path, #è®­ç»ƒæ•°æ®é›†è·¯å¾„
                                              imgsz, # è¾“å…¥å›¾åƒå°ºå¯¸
                                              batch_size // WORLD_SIZE, #æ¯æ‰¹å›¾åƒæ•°
                                              gs, # global_size
                                              single_cls, # æ˜¯å¦å•ç±»åˆ«
                                              hyp=hyp, # æ§åˆ¶æ¨¡å‹è®­ç»ƒçš„è¶…å‚æ•°
                                              augment=True, # æ˜¯å¦å›¾åƒå¢å¼º
                                              cache=None if opt.cache == 'val' else opt.cache, # æ•°æ®æ˜¯å¦éœ€è¦ç¼“å­˜ã€‚å½“è¢«è®¾ç½®ä¸º 'val' æ—¶ï¼Œè¡¨ç¤ºè®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨åŒä¸€ä¸ªæ•°æ®é›†
                                              rect=opt.rect, # æ˜¯å¦ç”¨çŸ©å½¢è®­ç»ƒæ–¹å¼
                                              rank=LOCAL_RANK, # åˆ†å¸ƒå¼è®­ç»ƒï¼Œè¡¨ç¤ºå½“å‰è¿›ç¨‹åœ¨èŠ‚ç‚¹ä¸­çš„æ’å
                                              workers=workers, #æŒ‡å®š DataLoader ä½¿ç”¨çš„å·¥ä½œçº¿ç¨‹æ•°
                                              image_weights=opt.image_weights, # å›¾åƒæƒé‡
                                              quad=opt.quad,
                                              prefix=colorstr('train: '),
                                              shuffle=True #æ˜¯å¦æ‰“ä¹±æ•°æ®é›†é¡ºåº)
    labels = np.concatenate(dataset.labels, 0)
    # å°†æ•°æ®é›†ä¸­æ‰€æœ‰æ ‡ç­¾æ•°æ®ï¼ˆå³ç±»åˆ«ä¿¡æ¯ï¼‰æŒ‰ç…§çºµå‘é¡ºåºæ‹¼æ¥æˆä¸€ä¸ªæ–°çš„ä¸€ç»´æ•°ç»„ labels
    # ç”¨äºåç»­åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œåˆ†ç±»å‡†ç¡®ç‡è®¡ç®—
    mlc = int(labels[:, 0].max())  # max label class è·å¾—æœ€å¤§ç±»åˆ«å€¼
    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'
    # åˆ¤æ–­æœ€å¤§ç±»åˆ«å·æ˜¯å¦å°äºç±»åˆ«æ•°é‡ï¼Œè‹¥å¤§äºç­‰äºåˆ™æŠ›å‡ºå¼‚å¸¸
 
    # Process 0
    if RANK in {-1, 0}:
        # éªŒè¯æ•°æ®é›†åŠ è½½å™¨
        val_loader = create_dataloader(val_path, # è·¯å¾„
                                       imgsz,  # å›¾åƒå°ºå¯¸
                                       batch_size // WORLD_SIZE * 2, # æ¯æ‰¹å›¾åƒæ•°é‡
                                       gs,  # å…¨å±€æ­¥æ•°
                                       single_cls, # æ˜¯å¦å•ç±»åˆ«
                                       hyp=hyp, # è¶…å‚æ•°
                                       cache=None if noval else opt.cache, # ç¼“å­˜æ–‡ä»¶çš„è·¯å¾„ï¼Œç”¨äºåŠ è½½ä¹‹å‰å¤„ç†è¿‡çš„å›¾åƒ
                                       rect=True, # æ˜¯å¦çŸ©å½¢éªŒè¯
                                       rank=-1, # åˆ†å¸ƒå¼è®­ç»ƒä¸­è¿›ç¨‹çš„æ’åï¼Œä¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
                                       workers=workers * 2, #æ•°æ®åŠ è½½å™¨ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼Œç”¨äºå¹¶è¡ŒåŠ è½½æ•°æ®
                                       pad=0.5, # å›¾åƒå¡«å……æ¯”ä¾‹ï¼Œç”¨äºå¤„ç†å›¾åƒå¤§å°ä¸ä¸€è‡´çš„æƒ…å†µ
                                       prefix=colorstr('val: ')# æ•°æ®åŠ è½½å™¨çš„åç§°)[0] 
 
        if not resume:
            if plots:
                plot_labels(labels, names, save_dir) # ä¿å­˜æ‰€æœ‰ç±»åˆ«çš„æ ‡ç­¾æ•°é‡ï¼Œç”Ÿæˆlables.png
 
            # Anchors
            if not opt.noautoanchor:
                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)
                # åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œè‡ªåŠ¨è®¡ç®—å¹¶æ˜¾ç¤ºå»ºè®®çš„é”šæ¡†å¤§å°å’Œæ¯”ä¾‹
            model.half().float()  # pre-reduce anchor precision
            # å°†æ¨¡å‹çš„æƒé‡ä»æµ®ç‚¹æ•°æ ¼å¼è½¬æ¢ä¸ºåŠç²¾åº¦æµ®ç‚¹æ•°æ ¼å¼
 
        callbacks.run('on_pretrain_routine_end')
        # åœ¨æ¯ä¸ªé¢„è®­ç»ƒè¿­ä»£ç»“æŸåæ‰§è¡Œ on_pretrain_routine_end å›è°ƒå‡½æ•°
        # ç”¨äºå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¿…è¦çš„è°ƒæ•´æˆ–è€…å¤„ç†
 
    # DDP mode å¤šgpuè®­ç»ƒ
    if cuda and RANK != -1:
        model = smart_DDP(model)
 
    # Model attributes æ¨¡å‹å±æ€§
    nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)
    # de_parallel(model) å‡½æ•°ç”¨äºå°†æ¨¡å‹è½¬æ¢ä¸ºå¯ä»¥åœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œè¿è¡Œçš„å½¢å¼
    # å‡½æ•°ä¼šå¯¹æ¨¡å‹çš„å‚æ•°è¿›è¡Œåˆ’åˆ†ï¼Œä½¿å¾—ä¸åŒçš„éƒ¨åˆ†å¯ä»¥å¹¶è¡Œåœ°è®¡ç®—
    # ä»£ç é€šè¿‡å– model çš„æœ€åä¸€ä¸ªæ¨¡å—çš„ nl å±æ€§ï¼Œè·å¾—æ£€æµ‹å±‚çš„æ•°é‡
    # æ ¹æ® nl çš„å€¼ï¼Œä»£ç æ›´æ–°äº†è¶…å‚æ•° hyp ä¸­çš„ä¸‰ä¸ªå€¼ï¼šboxï¼Œclsï¼Œobjï¼Œä»¥åŠ label_smoothing
    hyp['box'] *= 3 / nl  # scale to layers
    hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers
    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers
    hyp['label_smoothing'] = opt.label_smoothing
    # å› ä¸ºä¸åŒå±‚çš„è¾“å‡ºå°ºå¯¸ä¸åŒï¼Œä¸ºäº†ä¿è¯è¶…å‚æ•°åœ¨ä¸åŒå±‚ä¹‹é—´çš„ä¸€è‡´æ€§
    # è¿™é‡Œå¯¹ hyp ä¸­çš„ä¸‰ä¸ªè¶…å‚æ•°è¿›è¡Œäº†ç¼©æ”¾ï¼Œä½¿å¾—å®ƒä»¬ä¸å±‚æ•°å’Œç±»åˆ«æ•°é‡æˆæ­£æ¯”
    
    # ä»¥ä¸‹4è¡Œä»£ç çš„ä½œç”¨æ˜¯ä¸ºæ¨¡å‹çš„è®­ç»ƒåšå‡†å¤‡ï¼Œä¸ºæ¨¡å‹çš„å±æ€§èµ‹å€¼ï¼Œå¹¶è®¡ç®—å„ä¸ªç±»åˆ«çš„æƒé‡
    model.nc = nc  # attach number of classes to model æ•°æ®é›†ä¸­æ¨¡å‹æ•°é‡èµ‹å€¼ç»™æ¨¡å‹ncå±æ€§
    model.hyp = hyp  # attach hyperparameters to model åŠ è½½è¶…å‚æ•°ï¼Œè®­ç»ƒä½¿ç”¨
    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights
    # æ ¹æ®æ•°æ®é›†çš„æ ‡ç­¾è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™æ¨¡å‹çš„ class_weights å±æ€§
    # è¿™ä¸ªå±æ€§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºåŠ¨æ€è°ƒæ•´æŸå¤±å‡½æ•°ä¸­å„ä¸ªç±»åˆ«çš„æƒé‡ï¼Œä»è€Œæ›´åŠ å…³æ³¨é‡è¦çš„ç±»åˆ«
    model.names = names 
    # å°†æ•°æ®é›†ä¸­çš„ç±»åˆ«åç§°åˆ—è¡¨ names èµ‹å€¼ç»™æ¨¡å‹çš„ names å±æ€§ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å‡ºé€šé“å¯¹åº”çš„ç±»åˆ«åç§°
 
    # Start training  å¼€å§‹è®­ç»ƒ
    t0 = time.time() # è®°å½•æ—¶é—´
    nb = len(train_loader)  # number of batches batch_sizeçš„é•¿åº¦ï¼Œè¡¨ç¤ºä¸€å…±ä¼ å…¥äº†å‡ æ¬¡batch
    nw = max(round(hyp['warmup_epochs'] * nb), 100)  # number of warmup iterations, max(3 epochs, 100 iterations)
    # è®¡ç®—é¢„çƒ­æœŸçš„è¿­ä»£æ¬¡æ•°nwï¼Œè®­ç»ƒè¿‡ç¨‹å¼€å§‹æ—¶é€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œç›´åˆ°è®­ç»ƒè¿‡ç¨‹ç¨³å®š
    # é¢„çƒ­æœŸçš„ç›®çš„æ˜¯é¿å…æ¨¡å‹åœ¨åˆå§‹çš„è®­ç»ƒé˜¶æ®µè¿‡å¿«åœ°æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ï¼Œä»è€Œæé«˜æ¨¡å‹æ”¶æ•›åˆ°æ›´ä¼˜è§£çš„æ¦‚ç‡ã€‚
    # é¢„çƒ­æœŸçš„è¿­ä»£æ•°é‡åº”è¯¥æ ¹æ®æ•°æ®é›†çš„å¤§å°å’Œè¶…å‚æ•°çš„å…·ä½“è®¾ç½®è¿›è¡Œè°ƒæ•´ï¼Œä»¥å–å¾—æœ€ä½³æ•ˆæœã€‚
    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training
    last_opt_step = -1
    # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œlast_opt_step å˜é‡ç”¨äºè®°å½•ä¸Šä¸€æ¬¡è¿›è¡Œæ¢¯åº¦ä¸‹é™æ›´æ–°çš„è¿­ä»£æ¬¡æ•°
    # åœ¨è¿›è¡Œä¼˜åŒ–å™¨çŠ¶æ€æ¢å¤æ—¶ï¼Œå¯ä»¥ä½¿ç”¨è¯¥å˜é‡ä½œä¸ºå¼€å§‹è¿­ä»£æ¬¡æ•°ï¼Œå¹¶ç»§ç»­è®­ç»ƒã€‚
    # å¦‚æœæ²¡æœ‰ä¸Šä¸€æ¬¡çš„è¿­ä»£è®°å½•ï¼Œå¯ä»¥å°†è¯¥å˜é‡è®¾ç½®ä¸º -1ã€‚
    # è¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯åˆå§‹åŒ– last_opt_step å˜é‡ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºæ›´æ–°ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶è¿›è¡Œä¼˜åŒ–å™¨çŠ¶æ€æ¢å¤
    maps = np.zeros(nc)  # mAP per class åˆ›å»ºäº†ä¸€ä¸ªé•¿åº¦ä¸º nc çš„æ•°ç»„ mapsï¼Œå¹¶å°†å…¶æ‰€æœ‰å…ƒç´ åˆå§‹åŒ–ä¸º 0
    # åˆå§‹åŒ–ä¸€ä¸ªæ•°ç»„ mapsï¼Œä½œä¸ºè®°å½•æ¨¡å‹ mAP å€¼çš„å®¹å™¨ã€‚
    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)
    # åˆå§‹åŒ–ä¸€ä¸ªå…ƒç»„ resultsï¼Œä½œä¸ºè®°å½•æ¨¡å‹æ€§èƒ½æŒ‡æ ‡çš„å®¹å™¨ï¼Œæ–¹ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè®°å½•å’Œæ›´æ–°
    scheduler.last_epoch = start_epoch - 1  # do not move
    # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ scheduler çš„ last_epoch å±æ€§ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒå¼€å§‹æ—¶è‡ªåŠ¨è®¡ç®—å‡ºä¸‹ä¸€ä¸ª epoch çš„å­¦ä¹ ç‡
    scaler = torch.cuda.amp.GradScaler(enabled=amp)
    # åˆ›å»ºä¸€ä¸ª PyTorch è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨ scalerï¼Œå¹¶æ ¹æ®å¸ƒå°”å˜é‡ amp æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è¿›è¡Œé…ç½®
    # åœ¨è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨scalerå¯¹è±¡æ¥ç¼©æ”¾æ¢¯åº¦ï¼Œå¹¶æ‰§è¡Œæ­£å¸¸çš„åå‘ä¼ æ’­å’Œä¼˜åŒ–æ›´æ–°æ“ä½œ
    # ç”±äºé‡‡ç”¨äº†ä¼˜åŒ–çš„è®¡ç®—æ–¹å¼ï¼Œè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥è®©æ¨¡å‹åœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ›´å¿«åœ°å®Œæˆè®­ç»ƒ
    stopper, stop = EarlyStopping(patience=opt.patience), False
    # åˆ›å»ºä¸€ä¸ª EarlyStopping å¯¹è±¡ stopperï¼Œå¹¶å°†å¸ƒå°”å˜é‡ stop çš„å€¼åˆå§‹åŒ–ä¸º False
    # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒEarlyStopping æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„ç­–ç•¥ï¼Œç”¨äºé¿å…æ¨¡å‹è®­ç»ƒè¿‡åº¦æ‹Ÿåˆ
    # é€šè¿‡åœ¨æ¯ä¸ª epoch ç»“æŸåè®¡ç®—éªŒè¯é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œå¦‚æœæ¨¡å‹åœ¨è¿ç»­è‹¥å¹² epoch ä¸Šæ²¡æœ‰æ˜æ˜¾çš„æ”¹å–„ï¼Œå°±å¯ä»¥ç»ˆæ­¢è®­ç»ƒï¼Œé¿å…è¿›ä¸€æ­¥è¿‡åº¦æ‹Ÿåˆ
    # stopper å¯¹è±¡æ˜¯ä¸€ä¸ªEarlyStoppingç±»çš„å®ä¾‹ï¼Œç”¨äºåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæ€§èƒ½éªŒè¯å’Œç»ˆæ­¢è®­ç»ƒæ“ä½œ
    # patience å‚æ•°æŒ‡å®šäº†åœ¨è¿ç»­å¤šå°‘ä¸ª epoch ä¸Šæ²¡æœ‰å‡ºç°æ€§èƒ½æ”¹å–„æ—¶ï¼Œå°±ä¼šè§¦å‘ EarlyStopping ç­–ç•¥
    # stop å˜é‡è¡¨ç¤ºæ˜¯å¦éœ€è¦ç»ˆæ­¢è®­ç»ƒ
    compute_loss = ComputeLoss(model)  # init loss class
    # åˆ›å»ºä¸€ä¸ª ComputeLoss å¯¹è±¡ compute_lossï¼Œç”¨äºè®¡ç®—æ¨¡å‹åœ¨æ¯ä¸ª epoch ä¸Šçš„æŸå¤±å‡½æ•°å€¼ï¼Œ
    # å¹¶å°†æ¨¡å‹ model ä½œä¸ºå…¶è¾“å…¥å‚æ•°è¿›è¡Œåˆå§‹åŒ–
    callbacks.run('on_train_start')
    # æ‰“å°æ—¥å¿—ä¿¡æ¯
    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\n'
                f'Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\n'
                f"Logging results to {colorstr('bold', save_dir)}\n"
                f'Starting training for {epochs} epochs...')
    for epoch in range(start_epoch, epochs):  # å¼€å§‹ä¸€è½®ä¸€è½®è®­ç»ƒepoch ------------------------------------------------------------------
        callbacks.run('on_train_epoch_start')
        model.train() # å¯ç”¨äº†æ¨¡å‹çš„è®­ç»ƒæ¨¡å¼
        # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å¯ç”¨äº†ä¸€äº›ç‰¹å®šçš„æ¨¡å—ï¼Œæ¯”å¦‚ Dropout å’Œ Batch Normalizationï¼Œç”¨äºé˜²æ­¢æ¨¡å‹çš„è¿‡æ‹Ÿåˆå’Œç¨³å®šæ¢¯åº¦æ›´æ–°
        # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ç¦ç”¨äº†è¿™äº›ç‰¹å®šçš„æ¨¡å—ï¼Œä»¥ä¾¿å¯¹æ–°æ•°æ®è¿›è¡Œç²¾ç¡®çš„é¢„æµ‹
 
        # Update image weights (optional, single-GPU only)
        # ç”¨äºä»æ•°æ®é›†æ ‡ç­¾ä¸­è®¡ç®—å¹¶æ›´æ–°å›¾åƒæƒé‡
        # ä¸€æ‰¹ä¸€æ‰¹ä¼ æ•°æ®æ—¶ï¼Œéš¾è¯†åˆ«çš„ç›®æ ‡å¸Œæœ›å®ƒå¤šä¼ å…¥å‡ æ¬¡ï¼Œæ•°æ®é›†çš„æ¯å¼ å›¾ç‰‡åˆ†é…ä¸€ä¸ªé‡‡æ ·æƒé‡ï¼Œéš¾è¯†åˆ«çš„æƒé‡å¤§
        if opt.image_weights:
            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼Œä»¥ä¾¿å¹³è¡¡ç±»åˆ«ä¸å¹³è¡¡çš„æ•°æ®é›†
            # æ•°é‡å¤šæƒé‡å¤§ï¼ŒæŸä¸€ç±»çš„ä¸ç²¾ç¡®åº¦æ¯”è¾ƒé«˜ï¼Œå°±ä¼šç®—å‡ºä¸€ä¸ªæ¯”è¾ƒå¤§çš„ç±»åˆ«æƒé‡ï¼Œå¢åŠ å®ƒè¢«é‡‡æ ·çš„æ¦‚ç‡
            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å›¾åƒæƒé‡
            # ç®—å‡ºæ¥ç±»åˆ«æƒé‡ï¼Œä½†ä¼ ç»™æ¨¡å‹çš„æ˜¯å›¾ç‰‡ï¼Œè€Œä¸æ˜¯æ£€æµ‹æ¡†ï¼Œæ‰€ä»¥éœ€è¦æŠŠç±»åˆ«æƒé‡è½¬æ¢ä¸ºå›¾ç‰‡æƒé‡
            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx
            # é€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œè®­ç»ƒçš„è¿‡ç¨‹ï¼ŒåŸºäºæ¯ä¸ªæ ·æœ¬çš„å›¾åƒæƒé‡ï¼Œä»æ•°æ®é›†ä¸­éšæœºé€‰å–ç›¸åº”æ•°é‡çš„æ ·æœ¬
            # å¯ä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªç±»åˆ«éƒ½å¾—åˆ°äº†è¶³å¤Ÿçš„å…³æ³¨ï¼Œä»è€Œå‡å°‘ç±»åˆ«ä¸å¹³è¡¡çš„å½±å“
 
        # Update mosaic border (optional)
        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)
        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders
 
        mloss = torch.zeros(3, device=device)  # mean losses
        # åˆå§‹åŒ–mlosså˜é‡ï¼Œç”¨äºç¼“å­˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼ï¼Œå¹¶ä¸”å°†å…¶å­˜å‚¨åœ¨æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼Œä»¥ä¾¿æ›´é«˜æ•ˆåœ°è®¡ç®—å’Œæ›´æ–°
        if RANK != -1: # åˆ†å¸ƒå¼è®­ç»ƒ
            train_loader.sampler.set_epoch(epoch) # éœ€è¦è®¾ç½®samplerçš„éšæœºæ•°ç§å­ï¼Œä»¥ä¿è¯æ¯ä¸ªepochä¸­æ ·æœ¬çš„éšæœºæ€§
        pbar = enumerate(train_loader) # éå†train_loaderæ—¶è·å–è¿›åº¦æ¡ä¿¡æ¯
        LOGGER.info(('\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))
        # æ—¥å¿—è®°å½•
        if RANK in {-1, 0}:
            pbar = tqdm(pbar, total=nb, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar
        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        optimizer.zero_grad()
        # æ¸…ç©ºæ¢¯åº¦ç¼“å­˜ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡æ–°çš„æ¢¯åº¦è®¡ç®—
        # é€šå¸¸åœ¨æ¯ä¸ªbatchçš„è®­ç»ƒå¼€å§‹å‰ï¼Œæˆ‘ä»¬éƒ½ä¼šè°ƒç”¨è¿™ä¸ªæ–¹æ³•
        # ä»¥æ¸…ç©ºä¸Šä¸€ä¸ªbatchçš„æ¢¯åº¦ç¼“å­˜ï¼Œå¹¶å¼€å§‹å½“å‰batchçš„æ­£å‘ä¼ æ’­å’Œç›¸åº”çš„åå‘ä¼ æ’­è®¡ç®—
        for i, (imgs, targets, paths, _) in pbar:  # ä¸€æ‰¹ä¸€æ‰¹çš„å–æ•°æ®ï¼Œæ¯æ¬¡å–16 batch -------------------------------------------------------------
        # éå†train_loaderä¸­çš„æ‰€æœ‰æ•°æ®ï¼Œå¹¶è·å¾—å½“å‰batchçš„è¾“å…¥ã€æ ‡ç­¾ä»¥åŠå¯¹åº”çš„è·¯å¾„ä¿¡æ¯ï¼Œä»è€Œè¿›è¡Œæ¨¡å‹çš„è®­ç»ƒ
            callbacks.run('on_train_batch_start')
            ni = i + nb * epoch  # number integrated batches (since train start)
            # ä»ç¬¬0è½®å¼€å§‹ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ä¸€å…±è®­ç»ƒäº†å¤šå°‘æ‰¹æ•°æ®ï¼Œèµ·åˆ°è®°å½•æ‰¹æ¬¡çš„åŠŸèƒ½
            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0
            # å°†imgsè½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šè¿›è¡ŒåŠ é€Ÿè®¡ç®—
            # non_blocking=Trueè¡¨ç¤ºæ•°æ®è½¬ç§»è¿‡ç¨‹æ˜¯éé˜»å¡çš„ï¼Œè¿™æ„å‘³ç€è½¬ç§»æ“ä½œå°†åœ¨åå°å¼‚æ­¥è¿›è¡Œï¼Œè€Œä¸ä¼šå½±å“åç»­çš„ä»£ç æ‰§è¡Œã€‚è¿™æ ·å¯ä»¥æé«˜æ•°æ®è½¬ç§»å’Œæ¨¡å‹è®¡ç®—çš„æ•ˆç‡
            # .float() / 255æ˜¯å°†æ•°æ®ç±»å‹è½¬æ¢ä¸ºfloatï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–æ“ä½œï¼Œ
            # å°†åƒç´ å€¼ä»[0, 255]èŒƒå›´ç¼©æ”¾åˆ°[0, 1]èŒƒå›´å†…ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç”¨äºè®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹
 
            # Warmup çƒ­èº«è®­ç»ƒï¼Œå¼€å§‹ä½¿ç”¨å°å­¦ä¹ ç‡ï¼Œæ…¢æ…¢å‡åˆ°è®¾ç½®çš„å­¦ä¹ ç‡
            if ni <= nw:  # å½“å‰æ‰¹æ¬¡å°äºè®¾ç½®çš„wpæ‰€éœ€æ‰¹æ¬¡æ—¶ï¼Œä¸éœ€è¦æ›´æ–°å­¦ä¹ ç‡
                xi = [0, nw]  # x interp
                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)
                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())
                # è®¡ç®—åœ¨æ¢¯åº¦ç´¯ç§¯æ–¹å¼ä¸‹ï¼Œéœ€è¦ç´¯ç§¯å¤šå°‘ä¸ªbatchçš„æ¢¯åº¦
                for j, x in enumerate(optimizer.param_groups): # å¾ªç¯æ›´æ–°ä¼˜åŒ–å™¨å‚æ•°
                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 0 else 0.0, x['initial_lr'] * lf(epoch)])
                    # è®¡ç®—å½“å‰å‚æ•°ç»„çš„å­¦ä¹ ç‡lr
                    # éœ€è¦è®¡ç®—å½“å‰å­¦ä¹ ç‡ä¸‹é™çš„å¹…åº¦ï¼Œå¯ä»¥ä½¿å­¦ä¹ ç‡åœ¨è®­ç»ƒçš„åˆæœŸå¿«é€Ÿå¢åŠ ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ï¼›
                    # ç„¶åï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œé€æ¸å‡å°å­¦ä¹ ç‡ï¼Œé¿å…è®­ç»ƒè¿‡ç¨‹ä¸­éœ‡è¡ä¸æ”¶æ•›
                    # å¦‚æœå½“å‰ä¸ºç¬¬ä¸€æ¬¡å­¦ä¹ ç‡æ›´æ–°ï¼ˆå³jä¸º0ï¼‰ï¼Œä½¿ç”¨â€™warmup_bias_lrâ€™è¿™ä¸ªè¶…å‚æ•°ä½œä¸ºå­¦ä¹ ç‡çš„ä¸‹é™å¹…åº¦ï¼Œå¦åˆ™ä¸‹é™å¹…åº¦ä¸º0
                    # æ ¹æ®ä¸‹é™å¹…åº¦ï¼Œæ¥ç€å°†å…¶ä¹˜ä¸Šå½“å‰å‚æ•°ç»„çš„åˆå§‹å­¦ä¹ ç‡x[â€˜initial_lrâ€™]
                    # å¹¶ä½¿ç”¨ä¸€ä¸ªç±»å‹ä¸ºlfçš„å‡½æ•°å¯¹å­¦ä¹ ç‡è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚æœ€ç»ˆå¾—åˆ°çš„ç»“æœå°±æ˜¯å½“å‰å‚æ•°ç»„çš„å­¦ä¹ ç‡lr
                    if 'momentum' in x:
                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])
                    # æ ¹æ®å…¨å±€è®­ç»ƒæ­¥æ•°çš„å€¼æ¥åŠ¨æ€è°ƒæ•´å½“å‰å‚æ•°ç»„çš„åŠ¨é‡
                    # åœ¨å…¨å±€è®­ç»ƒæ­¥æ•°niå°äºé˜ˆå€¼nwæ—¶ï¼ŒåŠ¨é‡çš„å€¼é€æ¸å¢åŠ ï¼Œä»¥åŠ é€Ÿæ¨¡å‹å‚æ•°çš„æ›´æ–°è¿‡ç¨‹ï¼›
                    # å½“å…¨å±€è®­ç»ƒæ­¥æ•°niè¶…è¿‡é˜ˆå€¼nwæ—¶ï¼ŒåŠ¨é‡çš„å€¼é€æ¸å‡å°‘ï¼Œä»¥å‡ç¼“æ¨¡å‹å‚æ•°çš„æ›´æ–°é€Ÿåº¦ï¼Œé¿å…åœ¨minimaå¤„éœ‡è¡ã€‚
 
            # Multi-scale å¤šå°ºåº¦è®­ç»ƒ æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œå¯¹å›¾åƒè¿›è¡Œéšæœºå°ºå¯¸ç¼©æ”¾
            if opt.multi_scale:
                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size
                sf = sz / max(imgs.shape[2:])  # scale factor
            # è®­ç»ƒè¿‡ç¨‹éšæœºåŒ–å¾—åˆ°ä¸€ä¸ªæ¯”ä¾‹å› å­ ç”¨è¿™ä¸ªå› å­æ”¹å˜è¾“å…¥å›¾ç‰‡çš„å°ºåº¦ï¼Œèµ·åˆ°å¤šå°ºåº¦è®­ç»ƒçš„æ•ˆæœ
                if sf != 1: #é¦–å…ˆåˆ¤æ–­ç¼©æ”¾æ¯”ä¾‹sfæ˜¯å¦ç­‰äº1ï¼Œå¦‚æœä¸ç­‰äº1åˆ™è¯´æ˜éœ€è¦å¯¹å›¾åƒè¿›è¡Œç¼©æ”¾
                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)
                    # è®¡ç®—å‡ºç¼©æ”¾åçš„æ–°å°ºå¯¸nsï¼Œå°†å…¶å¯¹é½åˆ°gsçš„æ•´æ•°å€
                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)
                    # ä½¿ç”¨nn.functional.interpolateå‡½æ•°å¯¹å›¾åƒè¿›è¡Œæ’å€¼æ“ä½œï¼Œç¼©æ”¾åˆ°æ–°å°ºå¯¸ns
                    # æœ€ç»ˆå¾—åˆ°çš„imgså°±æ˜¯ç¼©æ”¾åçš„å›¾åƒæ•°æ®
 
            # Forward  å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(amp): # å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ
                pred = model(imgs)  # forward å°†å›¾ç‰‡è¾“å…¥ç½‘ç»œå‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹ç»“æœ
                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size
                # åˆ©ç”¨æ¨¡å‹é¢„æµ‹ä¿¡æ¯å’Œæ ‡æ³¨ä¿¡æ¯è®¡ç®—æŸå¤±å€¼å’ŒæŸå¤±ç»„ä»¶
                # å­—å…¸ç±»å‹çš„loss_itemsï¼Œè¯¥å­—å…¸è®°å½•äº†æ¨¡å‹çš„æŸå¤±å€¼ç›¸å¯¹äºä¸åŒç»„ä»¶çš„æŸå¤±è´¡çŒ®åº¦
                if RANK != -1:  # åˆ†å¸ƒå¼è®­ç»ƒ
                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode
                if opt.quad:
                    loss *= 4.
                # losså°†ä¼šä¹˜ä»¥4å€ä»¥å¢å¤§æƒ©ç½šé¡¹çš„å¼ºåº¦ï¼Œè¿›ä¸€æ­¥å½±å“æ¨¡å‹è®­ç»ƒçš„æ”¶æ•›é€Ÿåº¦å’Œç»“æœ
                # æŸäº›ä»»åŠ¡é€šå¸¸è¦æ±‚æ¨¡å‹åœ¨é«˜é¢‘ç‡ä¿¡æ¯çš„ä¿ç•™æ–¹é¢åšå¾—æ›´å¥½ï¼ŒåŒæ—¶å¾€å¾€è¦ä»˜å‡ºæ›´å¤§çš„è®¡ç®—ä»£ä»·
                # æ¨¡å‹çš„æŸå¤±å‡½æ•°åŠ å…¥ä¸€äº›æƒ©ç½šé¡¹ï¼Œæ¯”å¦‚L2æŸå¤±é¡¹ï¼Œä»¥æ­¤æ¥çº¦æŸé¢„æµ‹ç»“æœä¸çœŸå®å€¼ä¹‹é—´çš„å¹³æ»‘ç¨‹åº¦ã€‚
            # Backward  åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            # ä½¿ç”¨æ··åˆç²¾åº¦è¿›è¡Œåå‘ä¼ æ’­ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæŸå¤±å€¼é€šè¿‡scaler.scale()ä¹˜ä¸Šæ¯”ä¾‹å› å­ï¼Œä»¥ç¡®ä¿æ•°å€¼çš„ç¨³å®šæ€§
 
            # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
            if ni - last_opt_step >= accumulate:
            # å½“å‰è®­ç»ƒæ­¥æ•°ï¼ˆniï¼‰å’Œä¸Šä¸€æ¬¡ä¼˜åŒ–æ­¥æ•°ï¼ˆlast_opt_stepï¼‰ä¹‹å·®å¤§äºç­‰äºæŒ‡å®šçš„ç´¯ç§¯æ¢¯åº¦æ­¥æ•°ï¼ˆaccumulateï¼‰æ—¶ï¼Œæ‰§è¡Œä¼˜åŒ–å™¨çš„stepæ“ä½œï¼Œè¿›è¡Œå‚æ•°æ›´æ–°
                scaler.unscale_(optimizer)  # unscale gradients
                # æ‰§è¡Œäº†è‡ªåŠ¨æ··åˆç²¾åº¦çš„åå‘ä¼ æ’­æ“ä½œä¹‹åï¼Œä½¿æ¢¯åº¦è¿”å›åˆ°åŸå§‹çš„32ä½æµ®ç‚¹å‹æ ¼å¼
                #ï¼ˆåå‘è‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰ï¼Œä»¥ä¾¿è¿›è¡Œè¿›ä¸€æ­¥çš„æ¢¯åº¦å¤„ç†æˆ–ä¼˜åŒ–å™¨æ›´æ–°æ“ä½œã€‚
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
                # å¯¹æ¨¡å‹çš„æ¢¯åº¦è¿›è¡Œè£å‰ªï¼Œé¿å…å…¶è¿‡å¤§è€Œå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜
                # æ¢¯åº¦çˆ†ç‚¸æŒ‡çš„æ˜¯åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ç¥ç»ç½‘ç»œçš„æ¢¯åº¦å˜å¾—éå¸¸å¤§ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹çš„è®­ç»ƒå˜å¾—ä¸ç¨³å®š
                # è¿™ç§æƒ…å†µå¯èƒ½ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦è€—æ•£ï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœ
                # å½“æ¢¯åº¦çš„èŒƒæ•°ï¼ˆä¹Ÿç§°ä¸ºâ€œL2èŒƒæ•°â€ï¼‰è¶…è¿‡äº†æŒ‡å®šçš„æœ€å¤§å€¼max_normæ—¶ï¼Œ
                # è£å‰ªæ“ä½œå°†æŒ‰æ¯”ä¾‹ç¼©å°æ‰€æœ‰çš„æ¢¯åº¦ï¼Œä»¥ç¡®ä¿æ¢¯åº¦çš„å¤§å°ä¸è¶…è¿‡max_norm
                scaler.step(optimizer)  # optimizer.step æ›´æ–°ä¼˜åŒ–å™¨ä¸­çš„æƒé‡å‚æ•°
                scaler.update()
                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰åŠŸèƒ½æ›´æ–°scalerçš„çŠ¶æ€
                # ä½¿ç”¨scaler.step()å‡½æ•°æ›´æ–°å‚æ•°åï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡scaler.update()å‡½æ•°æ›´æ–°ç¼©æ”¾æ¯”ä¾‹å› å­ï¼Œä»¥ä¾¿åœ¨ä¸‹ä¸€æ¬¡batchä¸­ä½¿ç”¨
                optimizer.zero_grad()
                # å°†æ‰€æœ‰æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ä¿¡æ¯å½’é›¶ï¼Œä»¥ç¡®ä¿å½“å‰batchè®­ç»ƒä½¿ç”¨çš„æ˜¯æ–°çš„æ¢¯åº¦ä¿¡æ¯
                if ema:
                    ema.update(model)
                # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡æ›´æ–°åï¼ŒEMAç®—æ³•ä¼šå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œå¹³å‡å€¼è®¡ç®—ï¼Œ
                # å¹¶å°†å¹³å‡å€¼åº”ç”¨åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ä¸­ï¼Œä»¥å¹³æ»‘æ¨¡å‹å‚æ•°çš„å˜åŒ–å¹¶é˜²æ­¢è¿‡æ‹Ÿåˆ
                last_opt_step = ni
                # æŠŠlast_opt_stepæ›´æ–°ä¸ºå½“å‰çš„è®­ç»ƒæ­¥æ•°ï¼Œç”¨äºä¸‹ä¸€æ¬¡çš„ä¼˜åŒ–å™¨stepæ“ä½œ
 
            # Log æ—¥å¿—è®°å½•
            if RANK in {-1, 0}:
                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses
                # è¯¥å…¬å¼é€šè¿‡è®¡ç®—ä¹‹å‰æ‰€æœ‰çš„å¹³å‡å€¼å’Œå½“å‰batchä¸­çš„æŸå¤±å€¼æ¥æ›´æ–°æ–°çš„å¹³å‡å€¼
                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)
                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) %
                                     (f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))
                callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots)
                if callbacks.stop_training:  # è®­ç»ƒç»“æŸ
                    return
            # end batch ------------------------------------------------------------------------------------------------
 
        # Scheduler
        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers
        # è·å–ä¼˜åŒ–å™¨ï¼ˆoptimizerï¼‰ä¸­æ¯ä¸ªå‚æ•°ç»„ï¼ˆparam_groupsï¼‰çš„å­¦ä¹ ç‡ï¼ˆlrï¼‰
        scheduler.step() # ä¸€è½®æ‰€æœ‰æ‰¹æ¬¡è®­ç»ƒå®Œåï¼Œæ ¹æ®ä¹‹å‰çš„å­¦ä¹ ç‡æ›´æ–°ç­–ç•¥æ›´æ–°å­¦ä¹ ç‡
 
        if RANK in {-1, 0}:
            # mAP  è®¡ç®—å¹³å‡ç²¾åº¦
            callbacks.run('on_train_epoch_end', epoch=epoch)
            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])
            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop
            # åˆ¤æ–­å½“å‰æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªepochï¼Œä»¥ä¾¿åœ¨è®­ç»ƒç»“æŸæ—¶è¿›è¡Œç›¸å…³çš„æ“ä½œ
            # å½“å‰epochæ•°æ˜¯å¦ç­‰äºæ€»çš„epochæ•°å‡1ï¼ˆepochs-1ï¼‰ï¼Œæˆ–è€…è°ƒç”¨äº†æ—©åœæœºåˆ¶è€Œåœæ­¢äº†è®­ç»ƒ
            if not noval or final_epoch:  # Calculate mAP
            # åˆ¤æ–­novalå˜é‡æ˜¯å¦ä¸ºFalseï¼Œå¦‚æœæ˜¯Falseåˆ™è¡¨ç¤ºå½“å‰éœ€è¦è¿›è¡ŒéªŒè¯æ“ä½œ
            # åˆ¤æ–­äº†final_epochå˜é‡çš„å€¼ï¼Œå¦‚æœfinal_epochä¸ºTrueè¡¨ç¤ºå½“å‰æ˜¯æœ€åä¸€ä¸ªepochï¼Œåˆ™éœ€è¦è¿›è¡ŒéªŒè¯æ“ä½œ
                results, maps, _ = val.run(data_dict,
                                           batch_size=batch_size // WORLD_SIZE * 2,
                                           imgsz=imgsz,
                                           half=amp,
                                           model=ema.ema,
                                           single_cls=single_cls,
                                           dataloader=val_loader,
                                           save_dir=save_dir,
                                           plots=False,
                                           callbacks=callbacks,
                                           compute_loss=compute_loss)
 
            # Update best mAP æ›´æ–°æœ€ä¼˜map
            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]
            # æ ¹æ®ç»™å®šçš„è¯„ä»·æŒ‡æ ‡æ¥è®¡ç®—å½“å‰è®­ç»ƒçš„æ¨¡å‹å¯¹äºéªŒè¯é›†çš„è¡¨ç°
            stop = stopper(epoch=epoch, fitness=fi)  # early stop check æ£€æŸ¥æ˜¯å¦éœ€è¦æå‰åœæ­¢
            if fi > best_fitness: # å¦‚æœå½“å‰fiå¤§äºbest_fitness
                best_fitness = fi #å°±å°†å½“å‰fièµ‹å€¼ç»™best_fitness
            log_vals = list(mloss) + list(results) + lr
            # å°†å½“å‰epochçš„å„ä¸ªæŒ‡æ ‡ï¼ˆä¾‹å¦‚æŸå¤±å‡½æ•°å€¼ã€éªŒè¯ç»“æœã€å­¦ä¹ ç‡ï¼‰è®°å½•ä¸‹æ¥ï¼Œè¿›è¡Œå¯è§†åŒ–
            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)
 
            # Save model ä¿å­˜æ¨¡å‹
            if (not nosave) or (final_epoch and not evolve):  # if save
            # å¦‚æœnosaveä¸ºFalseï¼Œåˆ™è¡¨ç¤ºéœ€è¦ä¿å­˜æ¨¡å‹å‚æ•°
            # å¦‚æœfinal_epochä¸ºTrueï¼Œä¸”evolveä¸ºFalseï¼Œ
            # åˆ™è¡¨ç¤ºå½“å‰æ˜¯æœ€åä¸€ä¸ªepochï¼Œä¸”ä¸å¤„äºè¿›åŒ–ç®—æ³•ï¼ˆevolutionï¼‰ä¸­ï¼Œæ­¤æ—¶ä¹Ÿéœ€è¦ä¿å­˜æ¨¡å‹å‚æ•°ã€‚
                                            ckpt = {
                                                'epoch': epoch,
                                                'best_fitness': best_fitness,
                                                'model': deepcopy(de_parallel(model)).half(),
                                                'ema': deepcopy(ema.ema).half(),
                                                'updates': ema.updates,
                                                'optimizer': optimizer.state_dict(),
                                                'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None,
                                                'opt': vars(opt),
                                                'date': datetime.now().isoformat() #æ ‡è®°æ¨¡å‹å‚æ•°ä¿å­˜çš„æ—¶é—´}
                # å°†å½“å‰è®­ç»ƒçš„å„é¡¹å‚æ•°å­˜å‚¨åˆ°ä¸€ä¸ªå­—å…¸ckptä¸­ï¼Œä»¥ä¾¿åœ¨ä¿å­˜æ¨¡å‹å‚æ•°çš„æ—¶å€™ä½¿ç”¨
                # å°†ckptä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒç»“æŸåé‡æ–°ä½¿ç”¨æ¨¡å‹å‚æ•°
                # Save last, best and delete ä¿å­˜æœ€åä¸€æ¬¡ä»¥åŠæœ€å¥½çš„ä¸€æ¬¡
                torch.save(ckpt, last) # ä½¿ç”¨torch.saveå‡½æ•°å°†ckptå­—å…¸ä¿å­˜åˆ°æ–‡ä»¶ä¸­
                # lastè¡¨ç¤ºæŒ‡å®šçš„è·¯å¾„
                if best_fitness == fi:
                    torch.save(ckpt, best) # å¦‚æœæœ¬è½®æ‹Ÿåˆåº¦æœ€å¥½ï¼Œå°±ä¿å­˜best.pt
                if opt.save_period > 0 and epoch % opt.save_period == 0:
                    torch.save(ckpt, w / f'epoch{epoch}.pt')
                # æŒ‰ç…§ä¸€å®šçš„å‘¨æœŸè‡ªåŠ¨ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹å‚æ•°ï¼Œæ–¹ä¾¿æˆ‘ä»¬è¿›è¡Œåç»­çš„æ¨¡å‹è°ƒè¯•å’Œè¯„ä¼°
                del ckpt
                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)
 
        # EarlyStopping æ˜¯å¦æå‰åœæ­¢è®­ç»ƒ
        if RANK != -1:  # if DDP training  å¤šgpuè®­ç»ƒ
            broadcast_list = [stop if RANK == 0 else None]
            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
            if RANK != 0:
                stop = broadcast_list[0]
        if stop: # å¦‚æœæ»¡è¶³åœæ­¢è®­ç»ƒæ¡ä»¶ï¼Œåˆ™è·³å‡ºè®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            break  # must break all DDP ranks
 
        # end epoch ----------------------------------------------------------------------------------------------------
    # end training -----------------------------------------------------------------------------------------------------
    if RANK in {-1, 0}:
        LOGGER.info(f'\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')
        for f in last, best:
            if f.exists():
                strip_optimizer(f)  # strip optimizers
                # è®­ç»ƒç»“æŸä¹‹åï¼Œå°†æ¨¡å‹æ–‡ä»¶ä¸­çš„ä¼˜åŒ–å™¨ä¿¡æ¯åˆ é™¤ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥è¢«æ›´æ–¹ä¾¿åœ°åŠ è½½ã€æµ‹è¯•å’Œéƒ¨ç½²
                if f is best: #ç”¨æ•ˆæœæœ€å¥½çš„æƒé‡åœ¨éªŒè¯é›†ä¸Šå†è·‘ä¸€é å¹¶è¾“å‡ºæœ€ç»ˆçš„æŒ‡æ ‡ä»¥åŠæ¯ä¸€ç±»çš„å„ä¸ªæŒ‡æ ‡
                    LOGGER.info(f'\nValidating {f}...')
                    results, _, _ = val.run(
                                        data_dict,
                                        batch_size=batch_size // WORLD_SIZE * 2,
                                        imgsz=imgsz,
                                        model=attempt_load(f, device).half(),
                                        iou_thres=0.65 if is_coco else 0.60,  # best pycocotools results at 0.65
                                        single_cls=single_cls,
                                        dataloader=val_loader,
                                        save_dir=save_dir,
                                        save_json=is_coco,
                                        verbose=True,
                                        plots=plots,
                                        callbacks=callbacks,
                                        compute_loss=compute_loss)  # val best model with plots
                    # å¯¹æ¨¡å‹è¿›è¡ŒéªŒè¯ï¼Œå¹¶è·å¾—æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œä»è€Œå¯¹æ¨¡å‹è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–
                    if is_coco:
                        callbacks.run('on_fit_epoch_end', list(mloss) + list(results) + lr, epoch, best_fitness, fi)
 
        callbacks.run('on_train_end', last, best, plots, epoch, results)
 
    torch.cuda.empty_cache() # æ¸…ç©ºPyTorchåœ¨å½“å‰æ˜¾å­˜ä¸­æ‰€å ç”¨çš„ç¼“å­˜ç©ºé—´
    return results # è¿”å›ç»“æœ

def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')#weights: æ¨¡å‹é¢„è®­ç»ƒæƒé‡è·¯å¾„
    (1ï¼Œè‹¥åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨"â€“weights" å‚æ•°ï¼Œå¯æŒ‡å®šé¢„è®­ç»ƒæƒé‡æ–‡ä»¶(è·¯å¾„)ï¼›
     2ï¼Œè‹¥åœ¨å‘½ä»¤è¡Œä¸ä½¿ç”¨"â€“weights" å‚æ•°ï¼Œåˆ™é¢„è®­ç»ƒæƒé‡æ–‡ä»¶(è·¯å¾„)ä¸ºè‡ªå®šä¹‰çš„defaulté»˜è®¤å€¼ï¼›
     3ï¼Œè‹¥æ—¢ä½¿ç”¨å‘½ä»¤è¡Œ"â€“weights" å‚æ•°ï¼Œåˆè‡ªå®šä¹‰äº†defaulté»˜è®¤å€¼ï¼Œåˆ™æ¨¡å‹ä½¿ç”¨çš„æ˜¯å‘½ä»¤è¡Œ"â€“weights" å‚æ•°æŒ‡å®šçš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶(è·¯å¾„)
     4ï¼Œè‹¥ä¸è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä½¿ç”¨"â€“weights" å‚æ•°æŒ‡å®šä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼šâ€œâ€ï¼Œæˆ–è€…å°†defaulté»˜è®¤å€¼è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²ï¼šâ€œâ€)
    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')#cfg:æ¨¡å‹ç»“æ„æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºç©º
    (1ï¼Œåœ¨å·²ç»ä½¿ç”¨"â€“weights" å‚æ•°åŠ è½½äº†é¢„è®­ç»ƒæƒé‡çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä¸ä½¿ç”¨è¯¥å‚æ•°ï¼Œæ¨¡å‹ç»“æ„ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ä¸­ä¿å­˜çš„æ¨¡å‹ç»“æ„ï¼›
     2ï¼Œä¸ä½¿ç”¨"â€“weights" å‚æ•°ä½¿ç”¨"â€“cfg" å‚æ•°ï¼Œè¡¨ç¤ºæ¨¡å‹ä»å¤´å¼€å§‹è®­ç»ƒï¼Œä¸è¿›è¡Œé¢„è®­ç»ƒï¼›
     3ï¼Œâ€œâ€“weightsâ€ å‚æ•°å’Œ"â€“cfg" å‚æ•°å¿…é¡»è¦æœ‰ä¸€ä¸ªï¼Œä¸ç„¶ä»£ç ä¼šæŠ¥é”™)
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')#data:æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
    (1ï¼Œå¦‚æœæ²¡æœ‰æ£€æŸ¥åˆ°æ•°æ®é›†ï¼Œä»£ç ä¼šè‡ªåŠ¨ä¸‹è½½coco128æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥è‡ªå·±ä¸‹è½½ï¼›
     2ï¼ŒæŠŠyolov5å®˜æ–¹çš„æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é›†ä¸‹è½½éƒ¨åˆ†å†…å®¹ç»™æ³¨é‡Šæ‰ï¼Œä»£ç åˆ™ä¸ä¼šè‡ªåŠ¨ä¸‹è½½)
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')#hyp:è®­ç»ƒè¶…å‚æ•°é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument('--epochs', type=int, default=100, help='total training epochs')#epochs:è¡¨ç¤ºè®­ç»ƒæ•´ä¸ªè®­ç»ƒé›†çš„æ¬¡æ•°
    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')#batch-size:è®­ç»ƒæ‰¹é‡å¤§å°,è®­ç»ƒæ‰¹é‡å¤§å°è¡¨ç¤ºæ¯ä¸ª mini-batch ä¸­çš„æ ·æœ¬æ•°ï¼Œ
                                                                                                                     #batch-sizeè®¾ç½®ä¸ºnè¡¨ç¤ºä¸€æ¬¡æ€§ä»è®­ç»ƒé›†ä¸­è·å–nå¼ å›¾ç‰‡é€å…¥æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼›
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')#imgsz:æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯æ—¶è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸
    parser.add_argument('--rect', action='store_true', help='rectangular training')#rect:çŸ©å½¢è®­ç»ƒï¼Œé»˜è®¤å…³é—­(çŸ©å½¢è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå¯¹è¾“å…¥çš„çŸ©å½¢å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†ï¼Œé€šè¿‡ä¿æŒåŸå›¾é«˜å®½æ¯”è¿›è¡Œresizeåï¼Œ
                                                                                   å¯¹resizeåçš„å›¾ç‰‡è¿›è¡Œå¡«å……ï¼Œå¡«å……åˆ°32çš„æœ€å°æ•´æ•°å€ï¼Œç„¶åè¿›è¡ŒçŸ©å½¢è®­ç»ƒï¼Œå‡å°‘è®­ç»ƒæ—¶é—´ã€‚)
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')#resume:æ–­ç‚¹ç»­è®­ï¼Œé»˜è®¤å…³é—­(
    1ï¼Œæ–­ç‚¹ç»­è®­å°±æ˜¯ä»ä¸Šä¸€ä¸ªè®­ç»ƒä»»åŠ¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­è®­ç»ƒï¼Œç›´è‡³è®­ç»ƒå®Œæˆï¼›
    2ï¼Œå½“æ¨¡å‹æŒ‰æŒ‡å®šçš„epochè®­ç»ƒå®Œæˆåï¼Œåˆ™æ— æ³•è¿›è¡Œæ–­ç‚¹ç»­è®­ï¼›
    3ï¼Œéœ€è¦æ­é…"â€“weights" å‚æ•°ä½¿ç”¨ï¼ŒæŒ‡å®šè®­ç»ƒä¸­æ–­ä¿å­˜çš„æœ€åä¸€æ¬¡æ¨¡å‹æƒé‡æ–‡ä»¶
    nargs å‚æ•°è¡¨ç¤ºæ¥å—çš„å‘½ä»¤è¡Œå‚æ•°ä¸ªæ•°ã€‚å¯¹äº '?'ï¼Œå®ƒè¡¨ç¤ºæ¥å—é›¶ä¸ªæˆ–ä¸€ä¸ªå‚æ•°ã€‚åœ¨è¿™ä¸ªç‰¹å®šçš„æƒ…å†µä¸‹ï¼Œnargs='?' çš„ä½œç”¨æ˜¯æŒ‡å®š --resume è¿™ä¸ªå‘½ä»¤è¡Œå‚æ•°å¯ä»¥æœ‰é›¶ä¸ªæˆ–ä¸€ä¸ªå€¼)
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')#nosave:åªä¿ç•™æœ€åä¸€æ¬¡è®­ç»ƒçš„æƒé‡ï¼Œé»˜è®¤å…³é—­
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')#noval:åªå¯¹æœ€åä¸€æ¬¡è®­ç»ƒè¿›è¡ŒéªŒè¯ï¼Œé»˜è®¤å…³é—­
    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')#noautoanchor:å…³é—­è‡ªåŠ¨è®¡ç®—é”šæ¡†åŠŸèƒ½ï¼Œé»˜è®¤å…³é—­(
                                                                                         yolov5é‡‡ç”¨çš„æ˜¯kmeansèšç±»ç®—æ³•æ¥è®¡ç®—anchor boxçš„å¤§å°å’Œæ¯”ä¾‹ï¼Œæœ€ç»ˆè‡ªåŠ¨è®¡ç®—å‡ºä¸€ç»„æœ€åˆé€‚è®­ç»ƒçš„é”šæ¡†ã€‚)
    parser.add_argument('--noplots', action='store_true', help='save no plot files')#noplots:ä¸ä¿å­˜å¯è§†åŒ–æ–‡ä»¶
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')#evolve:ä½¿ç”¨è¶…å‚æ•°ä¼˜åŒ–ç®—æ³•è¿›è¡Œè‡ªåŠ¨è°ƒå‚ï¼Œé»˜è®¤å…³é—­(
    1ï¼Œyolov5é‡‡ç”¨é—ä¼ ç®—æ³•å¯¹è¶…å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œå¯»æ‰¾ä¸€ç»„æœ€ä¼˜çš„è®­ç»ƒè¶…å‚æ•°ï¼›
    2ï¼Œå¼€å¯åä¼ å…¥å‚æ•°nï¼Œè®­ç»ƒæ¯è¿­ä»£næ¬¡è¿›è¡Œä¸€æ¬¡è¶…å‚æ•°è¿›åŒ–ï¼›
    3ï¼Œå¼€å¯åä¸ä¼ å…¥å‚æ•°ï¼Œåˆ™é»˜è®¤ä¸ºconst=300)
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')#bucket:ä»è°·æ­Œäº‘ç›˜ä¸‹è½½æˆ–ä¸Šä¼ æ•°æ®(
    1ï¼Œè¯¥å‚æ•°ç”¨äºæŒ‡å®š gsutil bucket çš„åç§°ï¼Œå…¶ä¸­ gsutil æ˜¯ Google æä¾›çš„ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºè®¿é—® Google Cloud Storageï¼ˆGCSï¼‰æœåŠ¡ï¼›
    2ï¼ŒGCS æ˜¯ Google æä¾›çš„ä¸€ç§å¯¹è±¡å­˜å‚¨æœåŠ¡ï¼Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¦‚æœéœ€è¦ä½¿ç”¨ GCS ä¸­çš„æ•°æ®é›†ï¼Œå°±éœ€è¦æŒ‡å®š bucket çš„åç§°)
    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='image --cache ram/disk')#cache:ç¼“å­˜æ•°æ®é›†ï¼Œé»˜è®¤å…³é—­(
    1ï¼Œç¼“å­˜æ•°æ®é›†å›¾ç‰‡åˆ°å†…å­˜ä¸­ï¼Œè®­ç»ƒæ—¶æ¨¡å‹ç›´æ¥ä»å†…å­˜ä¸­è¯»å–ï¼ŒåŠ å¿«æ•°æ®åŠ è½½å’Œè®­ç»ƒé€Ÿåº¦
    2ï¼Œè‹¥"â€“cache"å‚æ•°æŒ‡å®šå€¼ï¼Œå¯ä»¥æŒ‡å®šçš„ï¼›å€¼ï¼šram/diskï¼›
    3ï¼Œè‹¥"â€“cache"å‚æ•°ä¸æŒ‡å®šå€¼ï¼Œåˆ™é»˜è®¤ä¸ºconst='ram')
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')#image-weights:å¯¹æ•°æ®é›†å›¾ç‰‡è¿›è¡ŒåŠ æƒè®­ç»ƒï¼Œé»˜è®¤å…³é—­(éœ€è¦æ­é…"â€“rect"å‚æ•°ä¸€èµ·ä½¿ç”¨)
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')#device:é€‰æ‹©è®­ç»ƒä½¿ç”¨çš„è®¾å¤‡å¤„ç†å™¨ï¼ŒCPUæˆ–è€…GPUï¼Œé»˜è®¤ä¸ºç©º(
    1ï¼Œé»˜è®¤ä¸ºç©ºæ—¶ï¼Œä»£ç ä¼šè¿›è¡Œè‡ªåŠ¨é€‰æ‹©ï¼Œè‹¥æ£€æŸ¥åˆ°è®¾å¤‡æœ‰GPUåˆ™ä¼˜å…ˆä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒï¼Œè‹¥æ²¡æœ‰åˆ™ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒï¼›
    2ï¼Œä½¿ç”¨GPUè®­ç»ƒæ—¶ï¼Œ0,1,2,3åˆ†åˆ«è¡¨ç¤ºç¬¬1ï¼Œ2ï¼Œ3ï¼Œ4å¼ GPU)
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')#multi-scale:å¤šå°ºåº¦è®­ç»ƒï¼Œé»˜è®¤å…³é—­(å¼€å¯å¤šå°ºåº¦è®­ç»ƒï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡è¾“å…¥å›¾ç‰‡ä¼šæ”¾å¤§æˆ–ç¼©å°50%)
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')#single-cls:å•ç±»åˆ«è®­ç»ƒï¼Œé»˜è®¤å…³é—­
    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')#optimizer:é€‰æ‹©è®­ç»ƒä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼Œé»˜è®¤ä½¿ç”¨SGD
                                                                                           (choices=[â€˜SGDâ€™, â€˜Adamâ€™, â€˜AdamWâ€™]è¡¨ç¤ºåªèƒ½é€‰æ‹©â€™SGDâ€™, â€˜Adamâ€™, 'AdamWâ€™è¿™ä¸‰ç§ä¼˜åŒ–å™¨ï¼Œä¹Ÿå¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„ä¼˜åŒ–å™¨)
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')#sync-bn:ä½¿ç”¨SyncBatchNorm(åŒæ­¥æ‰¹é‡å½’ä¸€åŒ–)ï¼Œåªæœ‰åœ¨ä½¿ç”¨DDPæ¨¡å¼(åˆ†å¸ƒå¼è®­ç»ƒ)æ—¶æœ‰æ•ˆï¼Œé»˜è®¤å…³é—­
    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')#workers:è®¾ç½®Dataloaderä½¿ç”¨çš„æœ€å¤§numworkersï¼Œé»˜è®¤è®¾ç½®ä¸º8
    (1ï¼ŒDataloaderä¸­numworkersè¡¨ç¤ºåŠ è½½å¤„ç†æ•°æ®ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åŠ è½½æ•°æ®æ—¶ï¼Œæ¯ä¸ªçº¿ç¨‹ä¼šè´Ÿè´£åŠ è½½å’Œå¤„ç†ä¸€æ‰¹æ•°æ®ï¼Œæ•°æ®åŠ è½½å¤„ç†å®Œæˆåï¼Œä¼šé€å…¥ç›¸åº”çš„é˜Ÿåˆ—ä¸­ï¼Œæœ€åä¸»çº¿ç¨‹ä¼šä»é˜Ÿåˆ—ä¸­è¯»å–æ•°æ®ï¼Œå¹¶é€å…¥GPUä¸­è¿›è¡Œæ¨¡å‹è®¡ç®—ï¼›
     2ï¼Œnumworkersä¸º0è¡¨ç¤ºä¸ä½¿ç”¨å¤šçº¿ç¨‹ï¼Œä»…ä½¿ç”¨ä¸»çº¿ç¨‹è¿›è¡Œæ•°æ®åŠ è½½å’Œå¤„ç†
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')#project:è®¾ç½®æ¯æ¬¡è®­ç»ƒç»“æœä¿å­˜çš„ä¸»è·¯å¾„åç§°
                                                                                              (ä½ æ¯æ¬¡è®­ç»ƒä¼šç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„å­æ–‡ä»¶å¤¹ï¼Œä¸»è·¯å¾„å°±æ˜¯å­˜æ”¾ä½ è¿™äº›å•ç‹¬å­æ–‡ä»¶å¤¹çš„åœ°æ–¹ï¼Œå¯ä»¥è‡ªå·±å‘½å)
    parser.add_argument('--name', default='exp', help='save to project/name')#name:è®¾ç½®æ¯æ¬¡è®­ç»ƒç»“æœä¿å­˜çš„å­è·¯å¾„åç§°
                                                                (å­è·¯å¾„æ˜¯ä¸Šé¢åœ¨â€™â€“projectâ€™ä¸­æåˆ°çš„æ¯æ¬¡è®­ç»ƒç”Ÿæˆçš„å•ç‹¬çš„å­æ–‡ä»¶å¤¹ï¼Œå¯ä»¥è‡ªå·±å‘½å,ä½ æ¯æ¬¡è®­ç»ƒç”Ÿæˆçš„æ¨¡å‹æƒé‡æ–‡ä»¶ã€å¯è§†åŒ–ç»“æœä»¥åŠå…¶å®ƒç»“æœæ–‡ä»¶ä¿å­˜çš„åœ°æ–¹)
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')#exist-ok:æ˜¯å¦è¦†ç›–åŒåçš„è®­ç»ƒç»“æœä¿å­˜è·¯å¾„ï¼Œé»˜è®¤å…³é—­ï¼Œè¡¨ç¤ºä¸è¦†ç›–
    (1ï¼Œä¸ä½¿ç”¨â€™â€“exist-okâ€™å‚æ•°æ—¶ï¼Œå¦‚æœâ€™â€“nameâ€™æŒ‡å®šçš„åç§°ä¸å˜ï¼Œæ¯”å¦‚â€™expâ€™ï¼Œæ¯æ¬¡è®­ç»ƒä¼šæŒ‰é¡ºåºæ–°å»ºæ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚exp1ã€exp2ã€exp3ã€â€¦ ã€expnï¼›
    2ï¼Œä½¿ç”¨â€™â€“exist-okâ€™å‚æ•°æ—¶ï¼Œå¦‚æœâ€™â€“nameâ€™æŒ‡å®šçš„åç§°ä¸å˜ï¼Œæ¯”å¦‚â€™expâ€™ï¼Œæ¯æ¬¡è®­ç»ƒåˆ™ä¸ä¼šæ–°å»ºæ–‡ä»¶å¤¹ï¼Œè®­ç»ƒç»“æœä¼šè¦†ç›–åŸå…ˆæ–‡ä»¶å¤¹ä¸­ä¿å­˜çš„æ‰€æœ‰ç»“æœ)
    parser.add_argument('--quad', action='store_true', help='quad dataloader')#quad:æ˜¯å¦ä½¿ç”¨quad dataloaderï¼Œé»˜è®¤å…³é—­
                                                     (quad dataloader æ˜¯ä¸€ç§æ•°æ®åŠ è½½å™¨ï¼Œå®ƒå¯ä»¥å¹¶è¡Œåœ°ä»ç£ç›˜è¯»å–å’Œå¤„ç†å¤šä¸ªå›¾åƒï¼Œå¹¶å°†å®ƒä»¬æ‰“åŒ…æˆå››å¼ å›¾åƒï¼Œä»è€Œå‡å°‘äº†æ•°æ®è¯»å–å’Œé¢„å¤„ç†çš„æ—¶é—´ï¼Œå¹¶æé«˜äº†æ•°æ®åŠ è½½çš„æ•ˆç‡)
    parser.add_argument('--cos-lr', action='store_true', help='cosine LR scheduler')#cos-lr:è®­ç»ƒå­¦ä¹ ç‡è¡°å‡ç­–ç•¥ä½¿ç”¨ä½™å¼¦é€€ç«ç­–ç•¥ï¼Œé»˜è®¤å…³é—­
                                                                                    (ä½™å¼¦é€€ç«ç­–ç•¥åœ¨è®­ç»ƒåˆæœŸåŠ å¿«å­¦ä¹ é€Ÿåº¦ï¼Œè®­ç»ƒåæœŸå‡å°å­¦ä¹ ç‡ï¼Œä»è€Œæ›´å¥½åœ°å­¦ä¹ æ•°æ®çš„åˆ†å¸ƒï¼Œé¿å…æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚)
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')#label-smoothing:è®­ç»ƒä½¿ç”¨æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    (1ï¼Œé»˜è®¤ä¸º0.0ï¼Œå³æ ‡ç­¾å¹³æ»‘ç­–ç•¥ä½¿ç”¨çš„epsilonä¸º0.0ï¼›
     2ï¼Œå°†æ ‡ç­¾å¹³æ»‘ç­–ç•¥ä½¿ç”¨çš„epsilonè®¾ç½®ä¸º0.1ï¼šè¡¨ç¤ºåœ¨æ¯ä¸ªæ ‡ç­¾çš„çœŸå®æ¦‚ç‡ä¸Šæ·»åŠ ä¸€ä¸ª epsilon=0.1çš„å™ªå£°ï¼Œä»è€Œä½¿æ¨¡å‹å¯¹æ ‡ç­¾çš„æ³¢åŠ¨æ›´åŠ é²æ£’ï¼›
     3ï¼Œâ€“label-smoothing å‚æ•°çš„å€¼åº”è¯¥æ ¹æ®å…·ä½“çš„æ•°æ®é›†å’Œæ¨¡å‹æ¥è°ƒæ•´ï¼Œä»¥è¾¾åˆ°æœ€ä¼˜çš„è®­ç»ƒæ•ˆæœ)
    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')#patience:è®­ç»ƒä½¿ç”¨EarlyStoppingç­–ç•¥ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                                               (â€˜â€“patienceâ€™å‚æ•°æŒ‡å®šä¸ºæ•´æ•°næ—¶ï¼Œè¡¨ç¤ºæ¨¡å‹åœ¨è®­ç»ƒæ—¶ï¼Œè‹¥è¿ç»­nä¸ªepochéªŒè¯ç²¾åº¦éƒ½æ²¡æœ‰æå‡ï¼Œåˆ™è®¤ä¸ºè®­ç»ƒå·²ç»è¿‡æ‹Ÿåˆï¼Œåœæ­¢è®­ç»ƒã€‚â€™â€“patienceâ€™å¯æ ¹æ®å…·ä½“çš„æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œè°ƒæ•´)
    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')#freeze:è®­ç»ƒä½¿ç”¨å†»ç»“è®­ç»ƒç­–ç•¥ï¼Œé»˜è®¤å…³é—­
    (1ï¼Œå†»ç»“è®­ç»ƒæ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“æ¨¡å‹ä¸­çš„æŸäº›å±‚ï¼Œå†»ç»“çš„å±‚ä¸è¿›è¡Œæƒé‡å‚æ•°æ›´æ–°ï¼›
     2ï¼ŒæŒ‡å®šâ€™0â€™æˆ–â€™-1â€™ï¼Œä¸å†»ç»“ä»»ä½•å±‚ï¼Œæ›´æ–°æ‰€æœ‰å±‚çš„æƒé‡å‚æ•°
     3ï¼ŒæŒ‡å®šnï¼Œå†»ç»“å‰n(0<n<=10)å±‚ï¼Œå³åªæ›´æ–°å‰nå±‚çš„æƒé‡å‚æ•°)
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')#save-period:æ¯è®­ç»ƒnä¸ªepochä¿å­˜ä¸€æ¬¡è®­ç»ƒæƒé‡ï¼Œé»˜è®¤å…³é—­
    (1ï¼Œn>0ï¼Œæ¯è®­ç»ƒnä¸ªepochä¿å­˜ä¸€æ¬¡è®­ç»ƒæƒé‡ï¼›
     2ï¼Œn<=0ï¼Œå…³é—­save-periodï¼Œåªä¿å­˜bestå’Œlastæƒé‡ã€‚)
    parser.add_argument('--seed', type=int, default=0, help='Global training seed')#seed:è®¾ç½®è®­ç»ƒä½¿ç”¨çš„å…¨å±€éšæœºç§å­(éšæœºç§å­å¯ä»¥ä¿è¯æ¯æ¬¡ç”Ÿæˆçš„ç»“æœéƒ½ä¸€è‡´ï¼Œä»è€Œæœ‰åˆ©äºä»£ç çš„å¯å¤ç°æ€§)
    parser.add_argument('--local_rank', type=int, default=-1, help='Automatic DDP Multi-GPU argument, do not modify')#local_rank:æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œé»˜è®¤å…³é—­

    # Logger arguments
    parser.add_argument('--entity', default=None, help='Entity')#entityï¼šç”¨äºæŒ‡å®šæ¨¡å‹å®ä½“çš„å‚æ•°
    (æ¨¡å‹å®ä½“å¯ä»¥æ˜¯ä¸€ä¸ªå®ä½“åç§°æˆ–å®ä½“ IDï¼Œé€šå¸¸ç”¨äºåœ¨å®ä½“å­˜å‚¨åº“ä¸­ç®¡ç†æ¨¡å‹çš„ç‰ˆæœ¬æ§åˆ¶å’Œè®°å½•ã€‚
    åœ¨ä½¿ç”¨å®ä½“å­˜å‚¨åº“æ—¶ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªå®ä½“æ¥å­˜å‚¨æ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒæ—¶æŒ‡å®šè¯¥å®ä½“ï¼Œè¿™æ ·è®­ç»ƒç»“æœå°±å¯ä»¥ä¸è¯¥å®ä½“ç›¸å…³è”å¹¶ä¿å­˜åˆ°å®ä½“å­˜å‚¨åº“ä¸­ã€‚
    è¯¥å‚æ•°é»˜è®¤å€¼ä¸º Noneï¼Œå¦‚æœæœªæŒ‡å®šå®ä½“ï¼Œåˆ™è®­ç»ƒç»“æœå°†ä¸ä¼šä¸ä»»ä½•å®ä½“ç›¸å…³è”)
    parser.add_argument('--upload_dataset', nargs='?', const=True, default=False, help='Upload data, "val" option')#upload_dataset:ç”¨äºä¸Šä¼ æ•°æ®é›†ï¼Œé»˜è®¤å…³é—­
    (1ï¼Œå¦‚æœå‘½ä»¤è¡Œæœªä½¿ç”¨â€™â€“upload_datasetâ€™å‚æ•°ï¼Œåˆ™é»˜è®¤å€¼ä¸ºdefault=Falseï¼Œè¡¨ç¤ºä¸ä¸Šä¼ æ•°æ®é›†ã€‚
     2ï¼Œå¦‚æœå‘½ä»¤è¡Œä½¿ç”¨â€™â€“upload_datasetâ€™å‚æ•°ï¼Œä½†æ²¡æœ‰ä¼ é€’å‚æ•°ï¼Œåˆ™é»˜è®¤å€¼ä¸ºconst=Trueï¼Œè¡¨ç¤ºä¸Šä¼ æ•°æ®é›†ã€‚
     3ï¼Œå¦‚æœå‘½ä»¤è¡Œä½¿ç”¨â€™â€“upload_datasetâ€™å‚æ•°ï¼Œå¹¶ä¸”ä¼ é€’äº†å‚æ•°â€™valâ€™ï¼Œåˆ™é»˜è®¤ä¸ºTrueï¼Œè¡¨ç¤ºè¦ä¸Šä¼ valæ•°æ®é›†)
    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval')#bbox_interval:æŒ‡å®šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯éš”å¤šå°‘ä¸ªepochè®°å½•ä¸€æ¬¡å¸¦æœ‰è¾¹ç•Œæ¡†çš„å›¾ç‰‡ï¼Œé»˜è®¤å…³é—­
    (1ï¼Œn>0ï¼Œæ¯éš”nä¸ªepochè®°å½•ä¸€æ¬¡å¸¦æœ‰è¾¹ç•Œæ¡†çš„å›¾ç‰‡ï¼›
     2ï¼Œn<=0ï¼Œå…³é—­â€“bbox_interval)
    parser.add_argument('--artifact_alias', type=str, default='latest', help='Version of dataset artifact to use')#artifact_alias:ç”¨äºæŒ‡å®šè¦ä½¿ç”¨çš„æ•°æ®é›†å·¥ä»¶çš„ç‰ˆæœ¬åˆ«åã€‚
                                                   (åœ¨ä½¿ç”¨MLFlowç­‰å·¥å…·è·Ÿè¸ªæ¨¡å‹è®­ç»ƒå’Œæ•°æ®é›†ç‰ˆæœ¬æ—¶ï¼Œä¼šç»™æ¯ä¸ªç‰ˆæœ¬åˆ†é…å”¯ä¸€çš„åˆ«åã€‚é€šè¿‡æŒ‡å®šæ­¤å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„æ•°æ®é›†å·¥ä»¶ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„æ•°æ®é›†å·¥ä»¶)

    return parser.parse_known_args()[0] if known else parser.parse_args()


def main(opt, callbacks=Callbacks()): # ä¸»å‡½æ•°
    # Checks 1.æ£€æŸ¥å·¥ä½œ
    if RANK in {-1, 0}: # RANKæ˜¯ä¸åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³çš„ï¼Œé»˜è®¤ä¸è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œå€¼ä¸º-1
        print_args(vars(opt)) # æ‰“å°ä¼ å…¥çš„å‚æ•°ä¿¡æ¯
        check_git_status()    # æ£€æŸ¥githubä»£ç æ˜¯å¦æ›´æ–°
        check_requirements()  # æ£€æŸ¥é¡¹ç›®æ‰€éœ€çš„ä¾èµ–åŒ…
 
    # Resume 2.ä»ä¸­æ–­æ¢å¤ï¼ˆæ¥ç€ä¸Šä¸€æ¬¡ç»§ç»­è®­ç»ƒï¼‰
    if opt.resume and not (check_wandb_resume(opt) or opt.evolve):  # resume from specified or most recent last.pt
    # å¦‚æœopt.resumeä¸ºTrueè¡¨ç¤ºéœ€è¦æ¢å¤ä¸­æ–­çš„ä»»åŠ¡ï¼Œ
    # check_wandb_resume(opt)è¿”å›Falseè¡¨ç¤ºè®­ç»ƒæ²¡æœ‰è¢«wandbæ¢å¤ï¼Œ
    # opt.evolveè¿”å›Falseè¡¨ç¤ºä¸æ˜¯åœ¨æ‰§è¡Œé—ä¼ ç®—æ³•
        last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())  #è·å–æœ€æ–°çš„è¿è¡Œç»“æœçš„æ–‡ä»¶è·¯å¾„ï¼Œå¹¶èµ‹å€¼ç»™lastå˜é‡
        opt_yaml = last.parent.parent / 'opt.yaml'  # train options yaml
        # æ„é€ ä¸€ä¸ªè·¯å¾„ï¼ŒæŒ‡å‘æœ€è¿‘è¿è¡Œç»“æœæ‰€åœ¨çš„è·¯å¾„çš„çˆ¶çº§ç›®å½•çš„çˆ¶çº§ç›®å½•ä¸‹çš„opt.yamlæ–‡ä»¶ã€‚
        opt_data = opt.data  # original dataset 
        # å°†ç¨‹åºæ‰€ä½¿ç”¨çš„æ•°æ®é›†å­˜å‚¨åˆ°å˜é‡opt_dataä¸­ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ã€‚
        if opt_yaml.is_file():  # æ£€æŸ¥opt.yamlæ˜¯å¦å­˜åœ¨
            with open(opt_yaml, errors='ignore') as f: # å­˜åœ¨åˆ™æ‰“å¼€è¯¥æ–‡ä»¶
                d = yaml.safe_load(f) # è§£ææ–‡ä»¶çš„å†…å®¹å¹¶ä»¥å­—å…¸çš„å½¢å¼åŠ è½½ï¼Œå­˜å‚¨åœ¨då˜é‡ä¸­
        else: #è‹¥opt.yamlä¸å­˜åœ¨
            d = torch.load(last, map_location='cpu')['opt']
            # è¯»å–æœ€è¿‘è¿è¡Œç»“æœçš„æ–‡ä»¶å¹¶åŠ è½½å…¶ä¸­ä¿å­˜çš„PyTorchæ¨¡å‹æ•°æ®åŠå…¶å®ƒä¿¡æ¯
        opt = argparse.Namespace(**d)  # replace
        # å°†ä¹‹å‰ä»æ–‡ä»¶ä¸­è¯»å–åˆ°çš„è®­ç»ƒé€‰é¡¹ä¿¡æ¯è½¬æ¢æˆä¸€ä¸ªargparse.Namespaceå¯¹è±¡
        # ä½¿ç”¨argparse.Namespace()æ„é€ ä¸€ä¸ªå‘½åç©ºé—´å¯¹è±¡opt
        # å¹¶ä¸”å°†ä¹‹å‰ä»æ–‡ä»¶ä¸­è¯»å–åˆ°çš„è®­ç»ƒé€‰é¡¹ä¿¡æ¯ä»¥å­—å…¸çš„å½¢å¼ä¼ ç»™Namespaceçš„æ„é€ å‡½æ•°
        # **æ˜¯ç”¨æ¥å¯¹ä¸€ä¸ªå­—å…¸è¿›è¡Œè§£åŒ…çš„æ“ä½œ
        # # replaceæ³¨é‡Šè¯´æ˜å°†optå¯¹è±¡æ›´æ–°ä¸ºä»æ–‡ä»¶ä¸­è¯»å–åˆ°çš„è®­ç»ƒé€‰é¡¹
        opt.cfg, opt.weights, opt.resume = '', str(last), True  # reinstate
        # opt.cfgå±æ€§è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²('')ï¼Œopt.weightså±æ€§è®¾ç½®ä¸ºlastæ–‡ä»¶åï¼Œopt.resumeå±æ€§è®¾ç½®ä¸ºTrue
        # è¿™äº›å±æ€§æŒ‡å®šé…ç½®æ–‡ä»¶çš„è·¯å¾„ã€æƒé‡æ–‡ä»¶çš„è·¯å¾„ä»¥åŠæ˜¯å¦æ¢å¤æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ç­‰é€‰é¡¹ã€‚
    if is_url(opt_data): # å°†æ–‡ä»¶è·¯å¾„ä¿å­˜åœ¨opt.dataå±æ€§ä¸­
            opt.data = check_file(opt_data)  # avoid HUB resume auth timeout
    else:
    # æ£€æŸ¥æ•°æ®ã€é…ç½®ã€è¶…å‚æ•°ã€æƒé‡æ–‡ä»¶ã€é¡¹ç›®è·¯å¾„æ˜¯å¦å­˜åœ¨
    opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \
        check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # è¿›è¡Œæ£€æŸ¥

    # è‡³å°‘æŒ‡å®š --cfg æˆ– --weights ä¸­çš„ä¸€ä¸ª
    assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'

    # å¦‚æœä½¿ç”¨è¿›åŒ–ç®—æ³•ï¼Œæ›´æ–°é¡¹ç›®è·¯å¾„ï¼Œå¹¶ç¦ç”¨æ¢å¤ï¼ˆresumeï¼‰
    if opt.evolve:
        if opt.project == str(ROOT / 'runs/train'):  # å¦‚æœé¡¹ç›®è·¯å¾„ä¸ºé»˜è®¤è·¯å¾„ï¼Œåˆ™æ”¹ä¸º runs/evolve
            opt.project = str(ROOT / 'runs/evolve')
        opt.exist_ok, opt.resume = opt.resume, False  # å°† resume å‚æ•°ä¼ é€’ç»™ exist_okï¼Œå¹¶ç¦ç”¨ resume

    # å¦‚æœ opt.name ä¸º 'cfg'ï¼Œå°†å…¶æ›´æ”¹ä¸ºæ¨¡å‹é…ç½®æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆä¸å¸¦åç¼€ï¼‰
    if opt.name == 'cfg':
        opt.name = Path(opt.cfg).stem  # ä½¿ç”¨ model.yaml æ–‡ä»¶çš„æ–‡ä»¶åä½œä¸ºåç§°

    # ä¿å­˜ç›®å½•è·¯å¾„ï¼Œä½¿ç”¨å¢é‡è·¯å¾„ä»¥é˜²å†²çª
    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))

   # DDP æ¨¡å¼
# é€‰æ‹©è®¾å¤‡
device = select_device(opt.device, batch_size=opt.batch_size)

# å¦‚æœ LOCAL_RANK ä¸ä¸º -1ï¼Œè¡¨ç¤ºå¤„äº DDP æ¨¡å¼
if LOCAL_RANK != -1:
    msg = 'ä¸å…¼å®¹ YOLOv5 çš„å¤š GPU DDP è®­ç»ƒ'
    
    # ç¡®ä¿ä¸ä½¿ç”¨ --image-weights é€‰é¡¹
    assert not opt.image_weights, f'--image-weights {msg}'
    
    # ç¡®ä¿ä¸ä½¿ç”¨ --evolve é€‰é¡¹
    assert not opt.evolve, f'--evolve {msg}'
    
    # ç¡®ä¿ --batch-size ä¸ä¸º -1ï¼ˆAutoBatch æ¨¡å¼éœ€è¦æœ‰æ•ˆçš„ --batch-sizeï¼‰
    assert opt.batch_size != -1, f'AutoBatch with --batch-size -1 {msg}, please pass a valid --batch-size'
    
    # ç¡®ä¿ --batch-size æ˜¯ WORLD_SIZE çš„å€æ•°
    assert opt.batch_size % WORLD_SIZE == 0, f'--batch-size {opt.batch_size} must be multiple of WORLD_SIZE'
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ CUDA è®¾å¤‡
    assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'
    
    # è®¾ç½®å½“å‰ CUDA è®¾å¤‡
    torch.cuda.set_device(LOCAL_RANK)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œè®­ç»ƒç»„
    dist.init_process_group(backend='nccl' if dist.is_nccl_available() else 'gloo',
                            timeout=timedelta(seconds=10800))


  # è®­ç»ƒ
if not opt.evolve:
    train(opt.hyp, opt, device, callbacks)

# è¿›è¡Œè¶…å‚æ•°æ¼”åŒ–
else:
    # è¶…å‚æ•°æ¼”åŒ–å…ƒæ•°æ®ï¼ˆå˜å¼‚å°ºåº¦ 0-1ï¼Œä¸‹é™ï¼Œä¸Šé™ï¼‰
    meta = {
        'lr0': (1, 1e-5, 1e-1),  # åˆå§‹å­¦ä¹ ç‡ (SGD=1E-2, Adam=1E-3)
        'lrf': (1, 0.01, 1.0),  # æœ€ç»ˆ OneCycleLR å­¦ä¹ ç‡ (lr0 * lrf)
        'momentum': (0.3, 0.6, 0.98),  # SGD åŠ¨é‡/Adam beta1
        'weight_decay': (1, 0.0, 0.001),  # ä¼˜åŒ–å™¨æƒé‡è¡°å‡
        'warmup_epochs': (1, 0.0, 5.0),  # é¢„çƒ­å‘¨æœŸï¼ˆå°æ•°ä¹Ÿå¯ä»¥ï¼‰
        'warmup_momentum': (1, 0.0, 0.95),  # é¢„çƒ­åˆå§‹åŠ¨é‡
        'warmup_bias_lr': (1, 0.0, 0.2),  # é¢„çƒ­åˆå§‹åç½®å­¦ä¹ ç‡
        'box': (1, 0.02, 0.2),  # ç›®æ ‡æ¡†æŸå¤±æƒé‡
        'cls': (1, 0.2, 4.0),  # ç±»åˆ«æŸå¤±æƒé‡
        'cls_pw': (1, 0.5, 2.0),  # ç±»åˆ« BCELoss æ­£æ ·æœ¬æƒé‡
        'obj': (1, 0.2, 4.0),  # ç›®æ ‡æŸå¤±æƒé‡ï¼ˆä¸åƒç´ ä¸€èµ·ç¼©æ”¾ï¼‰
        'obj_pw': (1, 0.5, 2.0),  # ç›®æ ‡ BCELoss æ­£æ ·æœ¬æƒé‡
        'iou_t': (0, 0.1, 0.7),  # IoU è®­ç»ƒé˜ˆå€¼
        'anchor_t': (1, 2.0, 8.0),  # é”šç‚¹å€æ•°é˜ˆå€¼
        'anchors': (2, 2.0, 10.0),  # è¾“å‡ºç½‘æ ¼çš„æ¯ä¸ªé”šç‚¹æ•°ï¼ˆ0 è¡¨ç¤ºå¿½ç•¥ï¼‰
        'fl_gamma': (0, 0.0, 2.0),  # focal loss gammaï¼ˆefficientDet é»˜è®¤ gamma=1.5ï¼‰
        'hsv_h': (1, 0.0, 0.1),  # å›¾åƒ HSV-Hue å¢å¼ºï¼ˆåˆ†æ•°ï¼‰
        'hsv_s': (1, 0.0, 0.9),  # å›¾åƒ HSV-Saturation å¢å¼ºï¼ˆåˆ†æ•°ï¼‰
        'hsv_v': (1, 0.0, 0.9),  # å›¾åƒ HSV-Value å¢å¼ºï¼ˆåˆ†æ•°ï¼‰
        'degrees': (1, 0.0, 45.0),  # å›¾åƒæ—‹è½¬ï¼ˆ+/- åº¦ï¼‰
        'translate': (1, 0.0, 0.9),  # å›¾åƒå¹³ç§»ï¼ˆ+/- åˆ†æ•°ï¼‰
        'scale': (1, 0.0, 0.9),  # å›¾åƒç¼©æ”¾ï¼ˆ+/- å¢ç›Šï¼‰
        'shear': (1, 0.0, 10.0),  # å›¾åƒå‰ªåˆ‡ï¼ˆ+/- åº¦ï¼‰
        'perspective': (0, 0.0, 0.001),  # å›¾åƒé€è§†ï¼ˆ+/- åˆ†æ•°ï¼‰ï¼ŒèŒƒå›´ 0-0.001
        'flipud': (1, 0.0, 1.0),  # å›¾åƒä¸Šä¸‹ç¿»è½¬ï¼ˆæ¦‚ç‡ï¼‰
        'fliplr': (0, 0.0, 1.0),  # å›¾åƒå·¦å³ç¿»è½¬ï¼ˆæ¦‚ç‡ï¼‰
        'mosaic': (1, 0.0, 1.0),  # å›¾åƒæ··åˆï¼ˆæ¦‚ç‡ï¼‰
        'mixup': (1, 0.0, 1.0),  # å›¾åƒæ··åˆï¼ˆæ¦‚ç‡ï¼‰
        'copy_paste': (1, 0.0, 1.0)}  # åˆ†å‰²å¤åˆ¶ç²˜è´´ï¼ˆ

       with open(opt.hyp, errors='ignore') as f:
    hyp = yaml.safe_load(f)  # åŠ è½½è¶…å‚æ•°å­—å…¸
    if 'anchors' not in hyp:  # åœ¨ hyp.yaml ä¸­æ³¨é‡Šæ‰äº† anchors
        hyp['anchors'] = 3
if opt.noautoanchor:
    del hyp['anchors'], meta['anchors']

opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # ä»…éªŒè¯/ä¿å­˜æœ€ç»ˆæ—¶æœŸ
# ei = [isinstance(x, (int, float)) for x in hyp.values()]  # å¯æ¼”åŒ–çš„ç´¢å¼•
evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'

if opt.bucket:
    # å¦‚æœå­˜åœ¨ï¼Œä¸‹è½½ evolve.csv
    subprocess.run([
        'gsutil',
        'cp',
        f'gs://{opt.bucket}/evolve.csv',
        str(evolve_csv), ])

for _ in range(opt.evolve):  # è¿›åŒ–çš„ä»£æ•°
    if evolve_csv.exists():  # å¦‚æœ evolve.csv å­˜åœ¨ï¼šé€‰æ‹©æœ€ä½³è¶…å‚æ•°å¹¶è¿›è¡Œå˜å¼‚
        # é€‰æ‹©çˆ¶ä»£
        parent = 'single'  # çˆ¶ä»£é€‰æ‹©æ–¹æ³•ï¼š'single' æˆ– 'weighted'
        x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)
        n = min(5, len(x))  # è¦è€ƒè™‘çš„å…ˆå‰ç»“æœçš„æ•°é‡
        x = x[np.argsort(-fitness(x))][:n]  # æœ€ä½³ n ä¸ªå˜å¼‚
        w = fitness(x) - fitness(x).min() + 1E-6  # æƒé‡ï¼ˆæ€»å’Œ > 0ï¼‰
        
        if parent == 'single' or len(x) == 1:
            # x = x[random.randint(0, n - 1)]  # éšæœºé€‰æ‹©
            x = x[random.choices(range(n), weights=w)[0]]  # æƒé‡é€‰æ‹©
        elif parent == 'weighted':
            x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # æƒé‡ç»„åˆ

               # å˜å¼‚
mp, s = 0.8, 0.2  # å˜å¼‚æ¦‚ç‡ï¼Œsigma
npr = np.random
npr.seed(int(time.time()))
g = np.array([meta[k][0] for k in hyp.keys()])  # å¢ç›Š 0-1
ng = len(meta)
v = np.ones(ng)

while all(v == 1):  # å˜å¼‚ç›´åˆ°å‘ç”Ÿå˜åŒ–ï¼ˆé˜²æ­¢é‡å¤ï¼‰
    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)

for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)
    hyp[k] = float(x[i + 7] * v[i])  # å˜å¼‚

# é™åˆ¶åœ¨èŒƒå›´å†…
for k, v in meta.items():
    hyp[k] = max(hyp[k], v[1])  # ä¸‹é™
    hyp[k] = min(hyp[k], v[2])  # ä¸Šé™
    hyp[k] = round(hyp[k], 5)  # æœ‰æ•ˆæ•°å­—

# è®­ç»ƒå˜å¼‚
results = train(hyp.copy(), opt, device, callbacks)
callbacks = Callbacks()

# å†™å…¥å˜å¼‚ç»“æœ
keys = ('metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95', 'val/box_loss',
        'val/obj_loss', 'val/cls_loss')
print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)

# ç»˜åˆ¶ç»“æœ
plot_evolve(evolve_csv)
LOGGER.info(f'è¶…å‚æ•°è¿›åŒ–ç»“æŸ {opt.evolve}generations\n'
            f"ç»“æœä¿å­˜åˆ° {colorstr('bold', save_dir)}\n"
            f'ä½¿ç”¨ç¤ºä¾‹: $ python train.py --hyp {evolve_yaml}')

def run(**kwargs):
    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')  
    opt = parse_opt(True)  # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œåˆ›å»ºOptå®ä¾‹
    for k, v in kwargs.items():
        setattr(opt, k, v)  # è®¾ç½®Optå®ä¾‹çš„å±æ€§ï¼Œç”¨å…³é”®å­—å‚æ•°è¦†ç›–é»˜è®¤å€¼
    main(opt)  # è°ƒç”¨ä¸»å‡½æ•°è¿›è¡Œè®­ç»ƒ
    return opt  # è¿”å›Optå®ä¾‹



if __name__ == "__main__":
    opt = parse_opt() # è°ƒç”¨parse_optå‡½æ•°ï¼Œè§£æç”¨æˆ·ä¼ å…¥çš„å‚æ•°ï¼Œå­˜å‚¨åˆ°optå˜é‡ä¸­
    # è¿™äº›å‚æ•°ç”¨äºä¼ å…¥å…¶å®ƒæ¨¡å—æˆ–å‡½æ•°
    main(opt)  # è°ƒç”¨ä¸»å‡½æ•°ï¼Œä¼ å…¥optå‚æ•°
